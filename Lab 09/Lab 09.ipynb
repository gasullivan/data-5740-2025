{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "yxn3ntjcj76jyax4fc5o",
   "authorId": "1043139642183",
   "authorName": "GASULLIVAN",
   "authorEmail": "sullivangregorya@wustl.edu",
   "sessionId": "4142aa68-8503-4477-ac5f-08e9c2d5074b",
   "lastEditTime": 1761499529397
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c6a70-cc57-4402-8880-fdfc5c2daaf1",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# Lab 09: Planning for Growth at Washington Cafe\n\nCongratulations! You've just gotten your dream student work-study job as a data analyst for Washington Cafe. As you can guess from the name, Washginton Cafe was started by a group of students at Washington University in St. Louis three years ago. At the time, they were ambitious sophomores. Because putting so much of their passion into Washington Cafe took time away from studies, they barely graduated. They love the restaurant, and it's been successful enough that they want to grow it. And they know enough from their studies that the best way to do this will involve gathering insights from the data they've been collecting for the past three years.\n\n**That's where you fit it!**"
  },
  {
   "cell_type": "markdown",
   "id": "afa50632-07cc-4c0c-917d-386c588107bc",
   "metadata": {
    "name": "_1_small_diner",
    "collapsed": false
   },
   "source": "## Chapter 1: The Small Diner\n\nYour first order of business is to just get some plans in place for managing the labor demand for the small diner that Washington Cafe currently is.  You've got weekly labor data from the past two years of business. Each month, it seems like the management team is scrambling to figure out the shift schedule and, more often than not, the dining room is either short on staff or staff are sitting aorund doing nothing. Not the best experience for diners or for the staff.\n\nStart with a simple Auto-Regressive (AR) model to forecase the \"typical\" labor needs. There really isn't much to go on at this point other than previous labor usage, so that's where we'll start."
  },
  {
   "cell_type": "code",
   "id": "fbb12c11-3d8d-41bb-b0ce-5becbaac133e",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# These ACF/PACF plotters from statsmodels are helpful\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# From statsmodels, we can load in useful time series analysis tools\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# And scikit-learn has useful tools for evaluating the usefulness of a model\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "638d15d2-82ce-4cda-a4fa-8bd2bcda51bb",
   "metadata": {
    "language": "python",
    "name": "data_1",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Since we have to read and setup data for each phase...\ndef read_data(file):\n    df = pd.read_csv(file, parse_dates=['week_start_date'])\n    # Convert the start date time and specify that the requency is\n    # \"weekly, starting on Monday\"\n    df = df.set_index('week_start_date').asfreq('W-MON')\n    df.drop(columns='restaurant', inplace=True)\n    return df\n\nlabor1 = read_data('washington_cafe_stage1_diner_2018_2020.csv')\nlabor1.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1545427e-f70a-4f91-95a6-14e8ef479199",
   "metadata": {
    "language": "python",
    "name": "plot_1",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "plt.rcParams[\"figure.figsize\"] = (10, 3)\n\n# Create a function to do all the time series plots\ndef ts_plots(df, title):\n    # Visualize time series raw\n    df.plot(title=title)\n    plt.show()\n    \n    # Calculate appropriate number of lags (max 40% of data length, capped at 52)\n    max_lags = min(52, len(df) // 2 - 1)\n    \n    # ACF/PACF\n    plot_acf(df, lags=max_lags)\n    plt.show()\n    \n    plot_pacf(df, lags=max_lags, method='ywm')\n    plt.show()\n\n# Do timeline, acf, and pacf\nts_plots(labor1, 'Stage 1 - Diner')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c0c1765-70ea-43bf-82d4-406dbc9d62db",
   "metadata": {
    "name": "plot_1_interpret",
    "collapsed": false
   },
   "source": "**Main Plot** - Doesn't seem to be much pattern in the labor hours worked each week. That's consistent with what we've been told: There's limited planning going on and people just work whatever shifts they want. Lots of shortage and lots of excess relative to the actual demand.\n\n**Auto Correlation Function** - The ACF tells us how closely correlated each data point is with the data point that lags by 1, 2, 3, ... 52 time periods. In this case:\n* The ACF drops off immediately\n* There doesn't appear to be any seasonality (with strong bumps as certain points)\n* Mostly what matters seems to be whatever happened yesterday\n\n**Partial Auto Correlation Function** - The PACF tells us how much each data point is correlated with the lagging data points after taking all other lagging data points into consideration.\n* The PACF drops off immediately.\n\n\nSo, we can conclude exactly what we expected, there aren't any other hidden patterns to the data. We can simply using a auto-regression model for our planning purposes."
  },
  {
   "cell_type": "code",
   "id": "3268835b-7267-4d9c-ae65-d0fb4ccca18a",
   "metadata": {
    "language": "python",
    "name": "model_1",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's reserve our last 10 weeks for the testing period and \n# the other 94 weeks before that for training\nlabor1_train = labor1.iloc[:-10]\nlabor1_test = labor1.iloc[-10:]\n\nar_model = AutoReg(labor1_train, lags=1).fit()\nar_model.summary()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b54d36d-fa08-4714-a00c-3c32bb6743d5",
   "metadata": {
    "language": "python",
    "name": "model_1_eval",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "pred1 = ar_model.predict(start=labor1_test.index[0], end=labor1_test.index[-1])\n\nmae = mean_absolute_error(labor1_test, pred1)\nrmse = math.sqrt(mean_squared_error(labor1_test, pred1))\n\nprint(f'Mean Absolute Error:     {mae}')\nprint(f'Root Mean Squared Error: {rmse}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6925bb31-c4ee-4485-81f8-9110147204a3",
   "metadata": {
    "language": "python",
    "name": "plot_1_predict",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Plot forecast vs actual\ndef plot_predict(df_train, df_test, df_pred, title):\n    plt.plot(df_train.index, df_train.values, label='train')\n    plt.plot(df_test.index, df_test.values, label='test')\n    plt.plot(df_pred.index, df_pred.values, label='forecast')\n    plt.title(title)\n    plt.legend(); plt.show()\n\nplot_predict(labor1_train, labor1_test, pred1, 'Stage 1 - AR Forecast')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b677e475-4136-479d-a386-23b9ff2ce312",
   "metadata": {
    "name": "predict1_interpret",
    "collapsed": false
   },
   "source": "The actual test data observations (orange) bounce all of the place just like the training data observations.  Our model doesn't know much better that to basically repeat whatever it saw yesterday.  So, you tell the management team to just look at the average over the past year and plan on that being the go-forward plan... for now."
  },
  {
   "cell_type": "markdown",
   "id": "8e316bbd-eb64-4341-a152-8a71de9ece78",
   "metadata": {
    "name": "_2_local_fave",
    "collapsed": false
   },
   "source": "## 2. Washington Cafe is becoming a local favorite\n\nAfter another year, you decide to reevalute the labor trends. You've had lots of other projects going on around favorite dishes, where patrons are coming from, cost of ingredients. But now it's time to look back at the labor trends again.  You decide to look at the ACF and PACF again as a starting point."
  },
  {
   "cell_type": "code",
   "id": "bafb79dd-a26a-49b8-8a79-57db6ee31228",
   "metadata": {
    "language": "python",
    "name": "data_2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "labor2 = read_data('washington_cafe_stage2_local_favorite_2018_2022.csv')\nlabor2.head()\n\nlabor2 = labor2[-104:]\nlabor2.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bcae38b5-2e82-463f-b13a-ebd27cf7b1e6",
   "metadata": {
    "language": "python",
    "name": "plot_2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ts_plots(labor2, 'Stage 2 - Local Favorite')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8c1d854-ab06-438a-8a8f-fdd73f67223d",
   "metadata": {
    "language": "python",
    "name": "model_2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's reserve our last 10 weeks for the testing period and \n# And we can see a clear change in the past 2 years, so let's just look at the past 2 years\nlabor2_train = labor2.iloc[:-10]\nlabor2_test = labor2.iloc[-10:]\n\n# This time, we'll use a moving average model\n# (0,0,1) means:\n# p = auto-regressive - 0 means none\n# d = differences - 0 means none\n# q = moving average - \n#     1 means use the error from the 1 previous forecast to correct the next\nma_model = ARIMA(labor2_train, order=(0,0,1)).fit()\n\nma_model.summary()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e79652b7-7892-4d20-93b1-69e21cd9abd7",
   "metadata": {
    "language": "python",
    "name": "model_2_eval",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "pred2 = ma_model.predict(start=labor2_test.index[0], end=labor2_test.index[-1])\n\nmae = mean_absolute_error(labor2_test, pred2)\nrmse = math.sqrt(mean_squared_error(labor2_test, pred2))\n\nprint(f'Mean Absolute Error:     {mae}')\nprint(f'Root Mean Squared Error: {rmse}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1c3c9d0-964a-423d-b56c-3abb69934bdd",
   "metadata": {
    "language": "python",
    "name": "plot_2_predict",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "plot_predict(labor2_train, labor2_test, pred2, 'Stage 2 - MA Forecast')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3643f9c1-69b1-4af8-ab66-0f3fa814d447",
   "metadata": {
    "name": "_3_boom",
    "collapsed": false
   },
   "source": "## Business is Booming!\n\nFinally, the business starts to take off and there's a major influx of new business. The hard work, some marketing genius, and great food has paid off. Let's use the data to see if we can predict the growth rather than just react to it after a few stressful shifts."
  },
  {
   "cell_type": "code",
   "id": "c571b815-0d3d-4e20-83f2-4c79ebbe5bc8",
   "metadata": {
    "language": "python",
    "name": "data_3"
   },
   "outputs": [],
   "source": "labor3 = read_data('washington_cafe_stage3_boom_2023.csv')\nlabor3.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f19857b-b258-4dbc-ae1d-f3441587a523",
   "metadata": {
    "language": "python",
    "name": "plot_3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "ts_plots(labor3, 'Stage 3 - Boom')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "743f65fe-46a0-4566-9bec-308fb30e3b94",
   "metadata": {
    "name": "plot_3_interpret",
    "collapsed": false
   },
   "source": "What's different?\n\n\nThere is a clear, strong **upward trend** in the data. Labor hours are consistently growing from around 1,300 hours per week at the start of 2023 to over 2,000 hours per week by the end of the year. This is completely different from Stages 1 and 2, which showed relatively flat patterns with random fluctuations.\n\n**Auto Correlation Function (ACF)**: The ACF shows a **slow, gradual decay** rather than dropping off quickly like in Stages 1 and 2. The correlation stays high for many lags before slowly declining. This shows signs of **non-stationarity** caused by a trend in the data. The data points are highly correlated with their recent past because they all on the same upward trajectory.\n\n**Partial Auto Correlation Function (PACF)**: The PACF shows a **significant spike at lag 1**, with the other lags being much smaller and within the confidence bands. This suggests that an autoregressive component of order 1 (AR(1)) would be appropriate.\n\n**Key Insight**: This data is **non-stationary** due to the strong trend. We need to use **differencing** to remove the trend and make the series stationary before we can model it effectively. This is why ARIMA models will perform much better than simple AR or MA models for this stage.\n\nThe models requires differencing due to the trend.\n"
  },
  {
   "cell_type": "code",
   "id": "e086e016-c9d0-472d-b865-8a03cf0155c4",
   "metadata": {
    "language": "python",
    "name": "model_3"
   },
   "outputs": [],
   "source": "#added some code to eliminate the pesky warning (didn't change the outcome, but makes me feel better\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=ConvergenceWarning)\n\nlabor3_train = labor3[:-10]\nlabor3_test = labor3[-10:]\n\n\n    # 'White noise': (0,0,0),\n    # 'Random walk': (0,1,0),\n    # 'Constant': (0,2,0),\n    # '1st-order regression': (1,0,0),\n    # '2nd-order regression': (2,0,0),\n    # 'Differenced 1st-order': (1,2,0),\n    # 'Simple exponential smoothing': (0,1,1),\n    # '1st-order moving average': (0,0,1),\n    # '2nd-order moving average': (0,0,2),\n    # 'ARMA': (1,0,1),\n    # 'ARIMA': (1,1,1),\n    # 'Damped-trend linear exponential smoothing': (1,1,2),\n    # 'Linear exponential smoothing 1': (0,2,1),\n    # 'Linear exponential smoothing 2': (0,2,2)\n\n\nplt.rcParams['figure.figsize'] = (10,6)\n\nparams = {\n    'White noise': (0,0,0),\n    'Random walk': (0,1,0),\n    'Constant': (0,2,0),\n    '1st-order regression': (1,0,0),\n    '2nd-order regression': (2,0,0),\n    'Differenced 1st-order': (1,2,0),\n    'Simple exponential smoothing': (0,1,1),\n    '1st-order moving average': (0,0,1),\n    '2nd-order moving average': (0,0,2),\n    'ARMA': (1,0,1),\n    'ARIMA': (1,1,1),\n    'Damped-trend linear exponential smoothing': (1,1,2),\n    'Linear exponential smoothing 1': (0,2,1),\n    'Linear exponential smoothing 2': (0,2,2)\n}\n\npreds = {}\n\nfor label, param in params.items():\n    try:\n        model = ARIMA(labor3_train, order=param).fit()\n        pred = model.predict(start=labor3_test.index[0], end=labor3_test.index[-1])\n        preds[label] = pred\n#added this to handle another annoying warning\n    except Exception as e:\n        print(f\"Could not fit {label}: {e}\")\n        continue\n\nplt.plot(labor3_train.index, labor3_train.values, label='train')\nplt.plot(labor3_test.index, labor3_test.values, label='test')\n\nfor label, pred in preds.items():\n    plt.plot(pred.index, pred.values, linestyle='dashed', label=(label + ' ' + str(params[label])))\n\nplt.title('Stage 3 — ARIMA Forecasts')\nplt.legend()\nplt.show()\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9af7acd-9424-4c67-92c7-157768a0b8c5",
   "metadata": {
    "language": "python",
    "name": "model_3_eval",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Select the best performing model - typically ARIMA(1,1,1) for trend data\nbest_model = ARIMA(labor3_train, order=(1,1,1)).fit()\npred3 = best_model.predict(start=labor3_test.index[0], end=labor3_test.index[-1])\n\nmae3 = mean_absolute_error(labor3_test, pred3)\nrmse3 = math.sqrt(mean_squared_error(labor3_test, pred3))\n\nprint(f'Mean Absolute Error:     {mae3}')\nprint(f'Root Mean Squared Error: {rmse3}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "debe2969-a79c-468d-9f97-0f440ce03366",
   "metadata": {
    "language": "python",
    "name": "plot_3_predict",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "plot_predict(labor3_train, labor3_test, pred3, 'Stage 3 - ARIMA(1,1,1) Forecast')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2e51679-4730-4ea2-9f16-5e1fd815e5fa",
   "metadata": {
    "language": "python",
    "name": "arima_all",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create a comparison table of all models tested\nresults = []\n\nfor label, param in params.items():\n    try:\n        model = ARIMA(labor3_train, order=param).fit()\n        pred = model.predict(start=labor3_test.index[0], end=labor3_test.index[-1])\n        mae = mean_absolute_error(labor3_test, pred)\n        rmse = math.sqrt(mean_squared_error(labor3_test, pred))\n        aic = model.aic\n        \n        results.append({\n            'Model': label,\n            'Order (p,d,q)': str(param),\n            'MAE': round(mae, 2),\n            'RMSE': round(rmse, 2),\n            'AIC': round(aic, 2)\n        })\n    except Exception as e:\n        continue\n\n# Create dataframe and sort by MAE\nresults_df = pd.DataFrame(results).sort_values('MAE')\nprint('Model Comparison for Stage 3 (sorted by MAE):')\nprint(results_df.to_string(index=False))\n\nprint(f'\\nBest Model: {results_df.iloc[0][\"Model\"]} with MAE = {results_df.iloc[0][\"MAE\"]}')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b55d45dc-9df2-44a2-b3d0-a6ae014d8dea",
   "metadata": {
    "name": "_4_fine_dining",
    "collapsed": false
   },
   "source": "## Into Fine Dining\n\nWith that huge growth in business last year, Washington Cafe has decided to transform into a fine dining establishment. As a result, there's more seasonal fluctuation in business (e.g., parent's weekend and holidays). Let's take a look and see if we can build a seasonal model, too."
  },
  {
   "cell_type": "code",
   "id": "2549f14b-dffb-4e83-bca6-f4bdf7876c50",
   "metadata": {
    "language": "python",
    "name": "data_4"
   },
   "outputs": [],
   "source": "labor4 = read_data('washington_cafe_stage4_fine_dining_2024_2026.csv')\nlabor4.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbbcf7ae-cdd8-4fb7-a0f0-3f129336cb8d",
   "metadata": {
    "language": "python",
    "name": "plot_4"
   },
   "outputs": [],
   "source": "ts_plots(labor4, 'Stage 4 - Fine Dining')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f7f21e9-2dbe-4190-91b3-53f7e88a34f7",
   "metadata": {
    "language": "python",
    "name": "plot_diff4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# The seasonality shows up in how high the ACF stays in the first chart\n# So, we need to difference over the past 52 weeks\n\n# Create seasonally differenced data\nlabor4_diff = labor4.diff(52).dropna()\n\nprint('Analyzing seasonal differencing (52-week period):')\nprint(f'Original data shape: {labor4.shape}')\nprint(f'Differenced data shape: {labor4_diff.shape}')\n\n# Plot the differenced data\nts_plots(labor4_diff, 'Stage 4 - Seasonally Differenced (52 weeks)')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "000fd547-4924-44b4-8d1d-63ffaa5863b9",
   "metadata": {
    "language": "python",
    "name": "model_4"
   },
   "outputs": [],
   "source": "\n# SARIMAX(p,d,q)(P,D,Q, s) — starting point (1,1,1)(1,1,1,52)\n\n#split into train and test\nlabor4_train = labor4.iloc[:-10]\nlabor4_test = labor4.iloc[-10:]\n\nprint(f'Training data: {len(labor4_train)} weeks')\nprint(f'Test data: {len(labor4_test)} weeks')\n\n# Fit SARIMAX model with seasonal components\n# (1,1,1) non-seasonal: AR(1), I(1), MA(1)\n# (1,1,1,52) seasonal: AR(1), I(1), MA(1) with 52-week period\nsarimax_model = SARIMAX(\n    labor4_train,\n    order=(1, 1, 1),\n    seasonal_order=(1, 1, 1, 52),\n    enforce_stationarity=False,\n    enforce_invertibility=False\n).fit(disp=False)\n\nprint('\\nSARIMAX Model Summary:')\nprint(sarimax_model.summary())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee948a1d-1477-4326-859f-bb472da55f02",
   "metadata": {
    "language": "python",
    "name": "model_4_eval"
   },
   "outputs": [],
   "source": "pred4 = sarimax_model.predict(start=labor4_test.index[0], end=labor4_test.index[-1])\n\nmae4 = mean_absolute_error(labor4_test, pred4)\nrmse4 = math.sqrt(mean_squared_error(labor4_test, pred4))\n\nprint(f'Mean Absolute Error:     {mae4}')\nprint(f'Root Mean Squared Error: {rmse4}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07e7973e-bee1-4bc3-9a68-796dfc8f243b",
   "metadata": {
    "language": "python",
    "name": "plot_4_predict",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "plot_predict(labor4_train, labor4_test, pred4, 'Stage 4 - SARIMAX Forecast')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f929f6f4-ad31-45ef-a029-fea85e66aed2",
   "metadata": {
    "name": "Conclusion",
    "collapsed": false
   },
   "source": "## In Summary: Our Beloved Washington Cafe\n\nOver the course of this analysis, we've witnessed Washington Cafe's remarkable transformation through four distinct stages, each requiring increasingly sophisticated forecasting methods:\n\n**Stage 1 - Small Diner (2018-2020)**: Random fluctuations with no clear pattern. A simple **AR(1) model** was sufficient since labor demand was essentially unpredictable, driven by ad-hoc scheduling rather than actual customer patterns.\n\n**Stage 2 - Local Favorite (2020-2022)**: Slight patterns began to emerge, but volatility remained high. A **Moving Average MA(1) model** helped capture short-term corrections based on recent forecast errors.\n\n**Stage 3 - Business Boom (2023)**: A clear upward trend emerged as the restaurant gained popularity. **ARIMA(1,1,1)** with first-order differencing successfully captured this growth trajectory. The differencing component (I) was critical for handling the non-stationary trend.\n\n**Stage 4 - Fine Dining (2024-2026)**: The transformation into a fine dining establishment introduced **seasonal patterns** tied to holidays, parent weekends, and other predictable events. **SARIMAX(1,1,1)(1,1,1,52)** captures both the underlying trend and the 52-week seasonal cycle.\n\n\n### The SARIMAX model's predictions show that:\n\n1. **Seasonality is real**: The model successfully identifies and forecasts the recurring patterns in labor demand. High-demand periods (likely holidays and special events) can now be anticipated rather than reacted to.\n\n2. **Planning is now possible**: Unlike the early diner days, Washington Cafe can now forecast labor needs weeks in advance with reasonable accuracy. The MAE and RMSE metrics indicate how close our predictions are to reality.\n\n3. **Staffing optimization**: Management can now:\n   - Hire additional staff ahead of busy seasons\n   - Schedule training during predictable slow periods\n   - Budget more accurately for labor costs\n   - Improve employee satisfaction through better scheduling\n\n### The Bigger Picture\n\nThis analysis demonstrates a fundamental principle: **as a business matures, its patterns become more predictable**. What started as chaotic, reactive scheduling evolved into data-driven workforce planning. The progression from AR → MA → ARIMA → SARIMAX mirrors the restaurant's journey from a scrappy startup to an established fine dining destination.\n\nThe key lesson for Washington Cafe's management: **invest in data collection and analysis early**. The patterns were always there in Stages 3 and 4, but without proper time series analysis, they would have remained hidden, leading to continued overstaffing or understaffing issues.\n\n### Next Steps\n\n- Continue monitoring model performance and refine parameters as more data becomes available\n- Consider adding exogenous variables (weather, local events, marketing campaigns) to further improve predictions\n- Extend this methodology to other aspects of the business (inventory, revenue, customer traffic)\n- Use these forecasts for strategic planning, such as expansion decisions or menu changes\n\nNow that Washington Cafe is running smoothly after some nice growth, we recommend our WashU founders enroll in the WashU McKelvey CAPS MDAA program so they can better operate their business.  Either that, or they should ask Paul, Greg, Luke or Nick to periodically conduct these analyses for them in exchange for an occasional free dining experience!"
  },
  {
   "cell_type": "code",
   "id": "7ad5a123-676c-4ec2-aae0-eebf35f61841",
   "metadata": {
    "language": "python",
    "name": "ForecastOneYear"
   },
   "outputs": [],
   "source": "#forecast out an additional 52 weeks (1 full year) beyond our test data\n#this extends the prediction beyond the boundaries of our actual data\n\n#get the last date in our dataset and forecast 52 weeks forward\nlast_date = labor4.index[-1]\nforecast_periods = 52\n\n#generate forecast starting from the day after our last observation\nforecast_extended = sarimax_model.forecast(steps=forecast_periods)\n\n#create a date range for the extended forecast\nforecast_index = pd.date_range(\n    start=last_date + pd.Timedelta(weeks=1),\n    periods=forecast_periods,\n    freq='W-MON'\n)\nforecast_extended.index = forecast_index\n\n#plot the extended forecast\nplt.figure(figsize=(12, 5))\nplt.plot(labor4.index, labor4.values, label='Historical Data', linewidth=2)\nplt.plot(forecast_extended.index, forecast_extended.values, \n         label='52-Week Forecast', linestyle='--', linewidth=2, color='red')\nplt.axvline(x=last_date, color='gray', linestyle=':', linewidth=1, label='Forecast Start')\nplt.title('Stage 4 - Extended 52-Week Forecast Beyond Available Data')\nplt.xlabel('Date')\nplt.ylabel('Labor Hours')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n#display forecast statistics\nprint(f'Forecast Summary for next 52 weeks:')\nprint(f'Starting from: {forecast_index[0].strftime(\"%Y-%m-%d\")}')\nprint(f'Ending at: {forecast_index[-1].strftime(\"%Y-%m-%d\")}')\nprint(f'Mean forecasted labor hours: {forecast_extended.mean():.2f}')\nprint(f'Min forecasted labor hours: {forecast_extended.min():.2f}')\nprint(f'Max forecasted labor hours: {forecast_extended.max():.2f}')\nprint(f'Standard deviation: {forecast_extended.std():.2f}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "662acbe1-2dc3-4501-b9f7-bad2755a4d56",
   "metadata": {
    "language": "python",
    "name": "ForecastUncertaintyBands"
   },
   "outputs": [],
   "source": "#get forecast with confidence intervals\nforecast_result = sarimax_model.get_forecast(steps=forecast_periods)\nforecast_mean = forecast_result.predicted_mean\nforecast_ci = forecast_result.conf_int()\n\n#setup the index\nforecast_mean.index = forecast_index\nforecast_ci.index = forecast_index\n\n#plot with confidence intervals\nplt.figure(figsize=(12, 5))\nplt.plot(labor4.index, labor4.values, label='Historical Data', linewidth=2)\nplt.plot(forecast_mean.index, forecast_mean.values, \n         label='52-Week Forecast', linestyle='--', linewidth=2, color='red')\nplt.fill_between(forecast_ci.index, \n                 forecast_ci.iloc[:, 0], \n                 forecast_ci.iloc[:, 1], \n                 color='red', alpha=0.2, label='95% Confidence Interval')\nplt.axvline(x=last_date, color='gray', linestyle=':', linewidth=1, label='Forecast Start')\nplt.title('Stage 4 - Extended Forecast with Confidence Intervals')\nplt.xlabel('Date')\nplt.ylabel('Labor Hours')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "677b84c0-55db-48ca-b500-d426ed44f6f0",
   "metadata": {
    "name": "ForecastCommentary",
    "collapsed": false
   },
   "source": "### Extended Forecast Analysis\n\n52-week forecast shows what Washington Cafe can expect for labor demand over the next full year:\n\n**Key Observations:**\n\n1. **Seasonal Pattern Continues**: The SARIMAX model projects that the seasonal cycle will continue, with peaks and valleys occurring at the same intervals as in the historical data (every 52 weeks).\n\n2. **Forecast Uncertainty**: The confidence interval (shaded area) widens as we move further into the future. This reflects increasing uncertainty as time goes on. The further out we go, the less certain we are with our forecast.\n\n3. **Long-term Planning**: Despite the uncertainty, this forecast provides valuable insights for:\n   - Annual budgeting for labor costs\n   - Identifying high-demand periods throughout the year for staff hiring (aligned to expected cycles)\n   - Planning major events or promotions during forecasted slow periods\n   - Setting realistic growth targets\n\n**Caution**: This forecast assumes the business will continue operating under similar conditions. Major changes (new menu, different hours, economic shifts, competitors) could invalidate these predictions. The model should be retrained regularly as new data becomes available."
  }
 ]
}