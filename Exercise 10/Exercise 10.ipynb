{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "e57kinqa623xuqnu65ci",
   "authorId": "2206420187358",
   "authorName": "GREGSULLIVAN",
   "authorEmail": "gregsullivan@ciosoglobal.com",
   "sessionId": "3671c854-703e-4d20-b365-d5638db985f3",
   "lastEditTime": 1762110786824
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a7ebe4-d155-485e-bc28-d039adc90ade",
   "metadata": {
    "name": "Exercise10",
    "collapsed": false
   },
   "source": "# Exercise 10: Logistic Regression Analysis\n## Framingham Heart Study - Coronary Heart Disease Risk Factors\n\n### Overview\nThis analysis examines risk factors for Coronary Heart Disease (CHD) using data from the \nFramingham Heart Study. I will build and evaluate a logistic regression model to identify \nsignificant predictors of CHD.\n\n### Dataset Information\n- **Source**: Framingham Heart Study, Levy (1999)\n- **Observations**: 4,695 patients\n- **Outcome**: CHD diagnosis (binary: 0 = No CHD, 1 = CHD)\n- **Predictors**: Sex, blood pressure, cholesterol, age, BMI, season\n"
  },
  {
   "cell_type": "markdown",
   "id": "30bc7c1b-9a34-4a0e-b1b2-fb929bd2a0c8",
   "metadata": {
    "name": "Questions1to3",
    "collapsed": false
   },
   "source": "## Questions 1-3: Research Question Overview\n\n### 1. What is the outcome variable?\nThe outcome variable is **CHD fate (chdfate)**, representing Coronary Heart Disease diagnosis:\n- **1** = CHD present\n- **0** = CHD absent\n\n### 2. What predictors are researchers interested in?\nCardiovascular risk factors being examined:\n- **sex**: Gender (1 = Male, 2 = Female)\n- **sbp**: Systolic Blood Pressure (mmHg)\n- **dbp**: Diastolic Blood Pressure (mmHg)  \n- **scl**: Serum Cholesterol Level (mg/dL)\n- **age**: Age (years)\n- **bmi**: Body Mass Index (kg/m²)\n- **month**: Month of year at baseline (will be converted to seasons)\n\n### 3. What is the hypothesis?\n**Hypothesis**: Cardiovascular risk factors (blood pressure, cholesterol, BMI, age, and sex) \nare associated with increased risk of developing Coronary Heart Disease."
  },
  {
   "cell_type": "code",
   "id": "12ba36f6-1028-4792-97df-d83f502bf454",
   "metadata": {
    "language": "python",
    "name": "ImportLibraries"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aea52417-2ec8-4644-832b-2cb9bb11bedc",
   "metadata": {
    "name": "Question4",
    "collapsed": false
   },
   "source": "## Question 4: Data Exploration and Summary Statistics\n\nI begin by loading the dataset and examining its structure, checking for missing values (second cell), \nand revealing descriptive statistics."
  },
  {
   "cell_type": "code",
   "id": "379191f8-4573-47d6-9814-2e1a8e5f83a3",
   "metadata": {
    "language": "python",
    "name": "LoadDataset"
   },
   "outputs": [],
   "source": "#open and read csv (which I saved from the xlsx)\nframingham_df = pd.read_csv('framingham_dataset_mod.csv')\n\n#quick look to see if we got it right\nprint(f\"Dataset shape: {framingham_df.shape}\")\nprint(f\"Observations: {framingham_df.shape[0]:,} | Variables: {framingham_df.shape[1]}\")\n\nframingham_df.head(10)\n\n#check data types\nframingham_df.dtypes\n\n#summary stats on the data\nframingham_df.describe()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf18c759-3af6-4f0d-aa6b-e70fc5bb9a93",
   "metadata": {
    "language": "python",
    "name": "MissingData",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#missing data assessment\n#identify and quantify missing values to determine the appropriate handling strategy\n\n#count missing values\nmissing_counts = framingham_df.isnull().sum()\nmissing_counts[missing_counts > 0]\n\n#create a missing values dataframe, accumulate to missing percentages\nmissing_pct = (framingham_df.isnull().sum() / len(framingham_df) * 100).round(2)\nmissing_summary_df = pd.DataFrame({\n    'Missing_Count': missing_counts[missing_counts > 0],\n    'Missing_Percentage': missing_pct[missing_counts > 0]\n})\n\n#show\nmissing_summary_df\n\n#calculate\ntotal_missing = missing_counts.sum()\ntotal_cells = framingham_df.shape[0] * framingham_df.shape[1]\n\n#some values are missing\nprint(f\"Total missing values: {total_missing}\")\nprint(f\"Percentage of all data: {(total_missing / total_cells * 100):.2f}%\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "634a2317-118d-4823-993d-cdaf2a3143a0",
   "metadata": {
    "name": "MissingDataStrategy",
    "collapsed": false
   },
   "source": "## Missing Data Strategy\nWith only 42 missing values (0.10% of total data), I will use **complete case analysis**.\n\nThis minimal data loss should not significantly impact our results. So, onward we go...\n"
  },
  {
   "cell_type": "code",
   "id": "8414e992-bca3-49aa-a81d-d9df722b9f12",
   "metadata": {
    "language": "python",
    "name": "CHDOutcomeDistribution"
   },
   "outputs": [],
   "source": "#CHD outcome distribution\n\n#determine chd factors\nchd_counts = framingham_df['chdfate'].value_counts()\nchd_prevalence = framingham_df['chdfate'].mean() * 100\n\n#show\nprint(\"CHD Outcome Distribution:\")\nprint(f\"  No CHD (0): {chd_counts[0]:,} ({chd_counts[0]/len(framingham_df)*100:.1f}%)\")\nprint(f\"  CHD (1): {chd_counts[1]:,} ({chd_counts[1]/len(framingham_df)*100:.1f}%)\")\nprint(f\"\\nCHD Prevalence: {chd_prevalence:.2f}%\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ca6bc5c-d268-48aa-8e67-7112d663bac9",
   "metadata": {
    "language": "python",
    "name": "SexDistribution"
   },
   "outputs": [],
   "source": "#same sex distribution\n\n#determine sex counts\nsex_counts = framingham_df['sex'].value_counts()\n\n#show\nprint(\"Sex Distribution:\")\nprint(f\"  Males (sex=1): {sex_counts[1]:,}\")\nprint(f\"  Females (sex=2): {sex_counts[2]:,}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "94d5d4ad-c19a-4aa6-a845-43e58e3ad939",
   "metadata": {
    "name": "Question5",
    "collapsed": false
   },
   "source": "## Question 5: Creating Seasonal Variables\n\nThe month variable needs transformation for regression modeling. I'll create four binary \nindicators for seasons, using winter as the reference category, as suggested.  [I love being able to format markdown cells]\n\n**Season Definitions:**\n- **Winter**: December (12), January (1), February (2)\n- **Spring**: March (3), April (4), May (5)\n- **Summer**: June (6), July (7), August (8)\n- **Fall**: September (9), October (10), November (11)"
  },
  {
   "cell_type": "code",
   "id": "8dd12d57-4a3e-46c1-9c98-2d6205d67a33",
   "metadata": {
    "language": "python",
    "name": "SeasonVariables"
   },
   "outputs": [],
   "source": "#create season variables from the month\nframingham_df['winter'] = framingham_df['month'].isin([12, 1, 2]).astype(int)\nframingham_df['spring'] = framingham_df['month'].isin([3, 4, 5]).astype(int)\nframingham_df['summer'] = framingham_df['month'].isin([6, 7, 8]).astype(int)\nframingham_df['fall'] = framingham_df['month'].isin([9, 10, 11]).astype(int)\n\n#load season variables into a separate dataframe\nseason_summary_df = pd.DataFrame({\n    'Winter': [framingham_df['winter'].sum()],\n    'Spring': [framingham_df['spring'].sum()],\n    'Summer': [framingham_df['summer'].sum()],\n    'Fall': [framingham_df['fall'].sum()]\n})\n\n#visual check\nseason_summary_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "856816de-ea12-4872-b470-0ef470f5a466",
   "metadata": {
    "name": "Question6",
    "collapsed": false
   },
   "source": "## Question 6: Initial Logistic Regression Model\n\nI'll fit an initial logistic regression model using all predictor variables (sex, sbp, dbp, \nscl, age, bmi) and seasonal indicators (spring, summer, fall), with winter as reference.\n\n**Note**: I'll exclude ID and month variables. ID is not a predictor, and I'll use season indicators rather than months."
  },
  {
   "cell_type": "code",
   "id": "84792331-ef26-4c46-9046-74782e28405a",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "#for all predictor SeasonVariables\npredictor_vars = ['sex', 'sbp', 'dbp', 'scl', 'age', 'bmi', 'spring', 'summer', 'fall']\n\n#start with the data as loaded\noriginal_size = len(framingham_df)\n\n#drop nulls into a new dataframe\nframingham_complete_df = framingham_df.dropna(subset=predictor_vars + ['chdfate'])\n\n#how many were removed by dropping the nulls\nremoved_count = original_size - len(framingham_complete_df)\n\n#show\nprint(f\"Original dataset size: {original_size:,}\")\nprint(f\"After removing missing data: {len(framingham_complete_df):,}\")\nprint(f\"Observations removed: {removed_count} ({removed_count/original_size*100:.2f}%)\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64be802e-5426-4eb5-9f79-697aa35ca877",
   "metadata": {
    "language": "python",
    "name": "PrepareModelData"
   },
   "outputs": [],
   "source": "#outcome and predictors\noutcome_var = framingham_complete_df['chdfate']\npredictor_data = framingham_complete_df[predictor_vars].copy()\npredictor_data_with_const = sm.add_constant(predictor_data)\n\n#show\nprint(f\"Outcome variable: chdfate (n={len(outcome_var):,})\")\nprint(f\"Predictor variables: {len(predictor_vars)}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50f305e1-4ac8-46c2-a5db-d16a16c56b21",
   "metadata": {
    "language": "python",
    "name": "InitialModelFit"
   },
   "outputs": [],
   "source": "#build logistic regression model (so easy in python!)\ninitial_logit_model = sm.Logit(outcome_var, predictor_data_with_const).fit()\n\n#display what we got\nprint(\"✓ Initial logistic regression model fitted successfully\")\nprint(f\"  Convergence status: {initial_logit_model.mle_retvals['converged']}\")\nprint(f\"  Iterations: {initial_logit_model.mle_retvals['iterations']}\")\n\n#show\nprint(initial_logit_model.summary())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ff4b399d-251b-45ca-bd20-4aa7d9b014bc",
   "metadata": {
    "name": "InitialModelResults",
    "collapsed": false
   },
   "source": "### Initial Model Results\n\nThe model converged successfully in 6 iterations. My key findings:\n- **Pseudo R²**: 0.077 (7.68% variance explained)\n- **Log-Likelihood**: -2,674.9\n- **LLR p-value**: 3.553e-90 (highly significant overall model)\n\n**The significant predictors (where p < 0.05):**\n- sex, sbp, scl, age, bmi\n\n**Non-significant predictors:**\n- dbp, seasonal variables"
  },
  {
   "cell_type": "markdown",
   "id": "a2f96459-e34a-4d94-bb1c-eecc7f2d4c32",
   "metadata": {
    "name": "Question7",
    "collapsed": false
   },
   "source": "## Question 7: Model Diagnostics\n\nI'll conduct comprehensive diagnostic checks to assess model assumptions and identify \npotential issues across two key questions, starting with:\n\n### 7a. Distribution of Predictor Variables\nCheck for skewness that might require transformation."
  },
  {
   "cell_type": "code",
   "id": "7f8acbf0-9e8b-4f10-80c9-534bb1cb5f27",
   "metadata": {
    "language": "python",
    "name": "CalculateSkewness"
   },
   "outputs": [],
   "source": "#continuous variables only\ncontinuous_vars = ['sbp', 'dbp', 'scl', 'age', 'bmi']\n\n#build the skewness list\nskewness_results = []\nfor var in continuous_vars:\n    skew_value = stats.skew(framingham_complete_df[var].dropna())\n    \n    if abs(skew_value) > 1:\n        assessment = \"HIGHLY SKEWED (suggest transforming)\"\n    elif abs(skew_value) > 0.5:\n        assessment = \"MODERATELY SKEWED (consider transforming)\"\n    else:\n        assessment = \"APPROXIMATELY SYMMETRIC (leave as is)\"\n\n    #build the skewness list\n    skewness_results.append({\n        'Variable': var,\n        'Skewness': round(skew_value, 3),\n        'Assessment': assessment\n    })\n\n#show skewness results\nskewness_df = pd.DataFrame(skewness_results)\nskewness_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6374644-0bd3-46d7-aacd-45c5f5d3d0ae",
   "metadata": {
    "language": "python",
    "name": "DistributionVisualization"
   },
   "outputs": [],
   "source": "#let's take a visual view of the various distributions\n\n#setup charts\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Distribution of Main Predictor Variables', fontsize=16, fontweight='bold')\n\nplot_vars = ['sbp', 'dbp', 'scl', 'age', 'bmi', 'sex']\n\n#build the plots' visualization\nfor idx, var in enumerate(plot_vars):\n    row = idx // 3\n    col = idx % 3\n    \n    axes[row, col].hist(framingham_complete_df[var], bins=30, edgecolor='black', alpha=0.7)\n    axes[row, col].set_title(var.upper(), fontweight='bold')\n    axes[row, col].set_xlabel('Value')\n    axes[row, col].set_ylabel('Frequency')\n    \n    skew_val = stats.skew(framingham_complete_df[var])\n    axes[row, col].text(0.02, 0.98, f'Skewness: {skew_val:.2f}', \n                        transform=axes[row, col].transAxes,\n                        verticalalignment='top',\n                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n#show\nplt.tight_layout()\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "70cd3c2f-3f01-4ac4-8f2f-a79d891b8a6b",
   "metadata": {
    "name": "Question7b",
    "collapsed": false
   },
   "source": "### 7b. Multicollinearity Assessment (VIF)\n\nFor the second part of our model assumptions testing, I'll check for multicollinearity using Variance Inflation Factors (VIF). High VIF values \n(>10) indicate problematic multicollinearity.\n\n**How to interpret VIF:**\n- VIF < 5: No concern\n- VIF 5-10: Moderate multicollinearity\n- VIF > 10: High multicollinearity (problematic)"
  },
  {
   "cell_type": "code",
   "id": "b66a52b2-aaa2-4373-a8f1-0ebc79dba06f",
   "metadata": {
    "language": "python",
    "name": "VIFCalculations"
   },
   "outputs": [],
   "source": "#again, our continuous variables\nvif_vars = ['sex', 'sbp', 'dbp', 'scl', 'age', 'bmi']\nvif_data = framingham_complete_df[vif_vars].copy()\n\n#build the VIF list\nvif_results = []\nfor i, var in enumerate(vif_vars):\n    vif_value = variance_inflation_factor(vif_data.values, i)\n    vif_results.append({'Variable': var, 'VIF': round(vif_value, 2)})\n\n#show our VIFs\nvif_df = pd.DataFrame(vif_results).sort_values('VIF', ascending=False)\nvif_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9c90980-ad97-4a1d-9571-452fd6e0a811",
   "metadata": {
    "language": "python",
    "name": "IdentifyHighVIFs"
   },
   "outputs": [],
   "source": "#find the high VIFs, build a dataframe for just those\nhigh_vif_df = vif_df[vif_df['VIF'] > 5]\n\n#show them, if there are any\nprint(f\"Variables with VIF > 5: {len(high_vif_df)}\")\nif len(high_vif_df) > 0:\n    print(\"\\n⚠ Multicollinearity detected:\")\n    for _, row in high_vif_df.iterrows():\n        print(f\"  - {row['Variable']}: VIF = {row['VIF']:.2f}\")\nelse:\n    print(\"\\n✓ No concerning multicollinearity detected\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c511c46b-9083-41bb-9a10-d3961c95c46b",
   "metadata": {
    "language": "python",
    "name": "CorrelationMatrix"
   },
   "outputs": [],
   "source": "#create the correlation matrix\ncorrelation_matrix = framingham_complete_df[vif_vars].corr()\ncorrelation_matrix.round(3)\n\n#build the correlation matrix\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\nax.set_title('Correlation Matrix of Predictor Variables', fontsize=14, fontweight='bold')\nplt.tight_layout()\n\n#show the correlation matrix\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85e488ba-a612-4b31-abf4-59e687bd98c3",
   "metadata": {
    "name": "FirstKeyFinding",
    "collapsed": false
   },
   "source": "### Key Finding: SBP and DBP Multicollinearity\n\nSBP and DBP show severe multicollinearity (VIF > 100, correlation r = 0.783). This is \nexpected as both measure blood pressure. \n\n**Remedial Action**: Remove DBP from the model, keeping SBP as it is more clinically \nrelevant and commonly used in cardiovascular risk assessment."
  },
  {
   "cell_type": "markdown",
   "id": "055e0f2d-1ee2-4103-890d-3cd173f3aeea",
   "metadata": {
    "name": "Question7c",
    "collapsed": false
   },
   "source": "### 7c. Linearity Assessment (Box-Tidwell Test)\n\nI'll test whether continuous predictors have linear relationships with the log-odds of CHD. \nThe Box-Tidwell test adds interaction terms between each predictor and its logarithm.\n\n**Interpretation**: If p-value > 0.05, the linearity assumption holds."
  },
  {
   "cell_type": "code",
   "id": "9481ae0b-2a58-4c57-b558-eb5988b5c19e",
   "metadata": {
    "language": "python",
    "name": "BoxTidwell"
   },
   "outputs": [],
   "source": "#Box-Tidwell setup and calcuoation\n\n#Box-Tidwell setup\nlinearity_predictors = ['sex'] + continuous_vars + ['spring', 'summer', 'fall']\nlinearity_data = framingham_complete_df[linearity_predictors].copy()\n\n#add log interaction terms\nfor var in continuous_vars:\n    linearity_data[f'{var}_log_interaction'] = (\n        linearity_data[var] * np.log(linearity_data[var] + 0.1)\n    )\n\n#add constant\nlinearity_data_with_const = sm.add_constant(linearity_data)\n\nprint(f\"✓ Created interaction terms for {len(continuous_vars)} continuous variables\")\n\n#build the Box-Tidwell model\nlinearity_model = sm.Logit(outcome_var, linearity_data_with_const).fit(disp=0)\n\nprint(\"✓ Box-Tidwell model fitted successfully\")\n\n#build a list of the Box-Tidwell results\nlinearity_results = []\nfor var in continuous_vars:\n    interaction_term = f'{var}_log_interaction'\n    coef = linearity_model.params[interaction_term]\n    pval = linearity_model.pvalues[interaction_term]\n    is_linear = 'Yes' if pval > 0.05 else 'No'\n    \n    linearity_results.append({\n        'Variable': var,\n        'Coefficient': round(coef, 4),\n        'P-value': round(pval, 4),\n        'Linear_Relationship': is_linear\n    })\n\n#show the Box-Tidwell results\nlinearity_df = pd.DataFrame(linearity_results)\nlinearity_df\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78cef92b-91bb-4592-a787-f6f4aa29eaed",
   "metadata": {
    "name": "LinearityAssessment",
    "collapsed": false
   },
   "source": "### Linearity Assessment Results\n\nAll continuous predictors show linear relationships with the log-odds of CHD (all p-values are greater than 0.05). No transformations are needed based on the linearity assumption."
  },
  {
   "cell_type": "markdown",
   "id": "5fe28710-3e18-4876-93a6-e7548e3b68bb",
   "metadata": {
    "name": "Assessments",
    "collapsed": false
   },
   "source": "### Linearity Assessment Results\n\nAll continuous predictors show linear relationships with the log-odds of CHD (all p-values \n> 0.05). No transformations are needed based on the linearity assumption.\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "d1db3869-e218-48ee-904f-4bef3461b05c",
   "metadata": {
    "name": "Question7d",
    "collapsed": false
   },
   "source": "### 7d. Outlier Assessment\n\nIdentify potential outliers and influential observations using:\n- **Pearson residuals**: Values > 3 indicate potential outliers\n- **Cook's Distance**: Values > 4/n indicate influential observations"
  },
  {
   "cell_type": "code",
   "id": "c967d30f-3ba2-43fe-97bd-0e4c13927704",
   "metadata": {
    "language": "python",
    "name": "PearsonResidualsAndCooks"
   },
   "outputs": [],
   "source": "#calculate Pearson residuals\npredicted_probs = initial_logit_model.predict(predictor_data_with_const)\npearson_residuals = initial_logit_model.resid_pearson\n\noutlier_threshold = 3\noutlier_mask = np.abs(pearson_residuals) > outlier_threshold\noutlier_count = outlier_mask.sum()\n\n#show Pearson residuals\nprint(f\"Observations with |Pearson residual| > {outlier_threshold}: {outlier_count}\")\nprint(f\"Percentage of data: {outlier_count/len(pearson_residuals)*100:.2f}%\")\n\n#calculate Cook's distance\nmodel_influence = initial_logit_model.get_influence()\ncooks_distance = model_influence.cooks_distance[0]\n\ncook_threshold = 4 / len(framingham_complete_df)\ninfluential_mask = cooks_distance > cook_threshold\ninfluential_count = influential_mask.sum()\n\n#show Cook's distance\nprint(f\"Influential observations (Cook's D > 4/n): {influential_count}\")\nprint(f\"Percentage of data: {influential_count/len(framingham_complete_df)*100:.2f}%\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0815fdad-b030-41ee-ba44-fad5721923ad",
   "metadata": {
    "language": "python",
    "name": "VisualizeOutlierDiagnostics"
   },
   "outputs": [],
   "source": "#now I'll visualize our outlier diagnostic information\n\n#setup visualizaton\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle('Diagnostic Plots for Outliers', fontsize=16, fontweight='bold')\n\n#residual plot\naxes[0, 0].scatter(predicted_probs, pearson_residuals, alpha=0.5)\naxes[0, 0].axhline(y=0, color='r', linestyle='--')\naxes[0, 0].axhline(y=3, color='orange', linestyle='--', alpha=0.5)\naxes[0, 0].axhline(y=-3, color='orange', linestyle='--', alpha=0.5)\naxes[0, 0].set_xlabel('Predicted Probability')\naxes[0, 0].set_ylabel('Pearson Residuals')\naxes[0, 0].set_title('Residual Plot')\n\n#Q-Q plot\nstats.probplot(pearson_residuals, dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title('Q-Q Plot')\n\n#Cook's distance\naxes[1, 0].stem(range(len(cooks_distance)), cooks_distance, markerfmt=',', basefmt=\" \")\naxes[1, 0].axhline(y=cook_threshold, color='r', linestyle='--', label=\"Threshold (4/n)\")\naxes[1, 0].set_xlabel('Observation Index')\naxes[1, 0].set_ylabel(\"Cook's Distance\")\naxes[1, 0].set_title(\"Cook's Distance\")\naxes[1, 0].legend()\n\n#leverage vs residuals\nleverage_values = model_influence.hat_matrix_diag\naxes[1, 1].scatter(leverage_values, pearson_residuals, alpha=0.5)\naxes[1, 1].axhline(y=0, color='r', linestyle='--')\naxes[1, 1].set_xlabel('Leverage')\naxes[1, 1].set_ylabel('Pearson Residuals')\naxes[1, 1].set_title('Leverage vs Residuals')\n\n#show\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4662c8ef-1622-4ec8-aba9-20d57ce81317",
   "metadata": {
    "name": "OutlierConclusion",
    "collapsed": false
   },
   "source": "### Outlier Assessment Conclusion\n\n- **Pearson residuals > 3**: Only 8 observations (0.17%)\n- **Influential observations**: 98 observations (2.11%)\n\nThe number of outliers is minimal and within expected ranges for a dataset of this size. \nThese observations represent valid data points and will be retained."
  },
  {
   "cell_type": "markdown",
   "id": "7f357c5b-4760-47f3-adf5-94c21165f6dc",
   "metadata": {
    "name": "Question7ePredictorOutcomes",
    "collapsed": false
   },
   "source": "### 7e. Adequate Outcomes Per Predictor Category\n\nVerifying that we have at least 5 CHD outcomes in each category of the sex variable. \nThis ensures stable coefficient estimation."
  },
  {
   "cell_type": "code",
   "id": "1952276d-b447-4f45-a075-2395a0375d71",
   "metadata": {
    "language": "python",
    "name": "BySex"
   },
   "outputs": [],
   "source": "#CHD by sex crosstab\nsex_chd_crosstab = pd.crosstab(\n    framingham_complete_df['sex'], \n    framingham_complete_df['chdfate'], \n    margins=True\n)\n\n#setup\nsex_chd_crosstab\n\n#outcomes per sex\nfor sex_value in framingham_complete_df['sex'].unique():\n    chd_count = len(framingham_complete_df[\n        (framingham_complete_df['sex'] == sex_value) & \n        (framingham_complete_df['chdfate'] == 1)\n    ])\n    sex_label = \"Male\" if sex_value == 1 else \"Female\"\n    status = \"✓ Sufficient\" if chd_count >= 5 else \"✗ Insufficient\"\n    \n    print(f\"{sex_label} (sex={sex_value}): {chd_count} CHD cases - {status}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b1ab8ec7-8d3a-4b3a-af1d-b1ffc384fe5b",
   "metadata": {
    "name": "CheckSexViability",
    "collapsed": false
   },
   "source": "Both sex categories have far more than the minimum 5 required outcomes, ensuring stable \ncoefficient estimation (Males: 818 cases, Females: 645 cases).\n"
  },
  {
   "cell_type": "markdown",
   "id": "0454b184-0181-4561-bedb-d69adbc997e5",
   "metadata": {
    "name": "Question8",
    "collapsed": false
   },
   "source": "## Question 8: Addressing Issues and Refitting Model\n\n### Summary of Diagnostic Findings\n\nThe diagnostic process identified one primary issue requiring remediation:\n\n**Multicollinearity**: Severe multicollinearity between SBP and DBP (VIF > 100, r = 0.783)\n\n### Remedial Action\n\n**I suggest removing DBP** from the model and retain SBP.\n\n**My rationale**: \n- SBP is more clinically relevant for cardiovascular disease prediction\n- SBP is the standard measure used in clinical practice\n- High correlation means both variables measure the same underlying construct"
  },
  {
   "cell_type": "code",
   "id": "6458c5f2-569c-49a6-b246-bc3d4c33d2fa",
   "metadata": {
    "language": "python",
    "name": "FinalModelBuild"
   },
   "outputs": [],
   "source": "#based on our diagnostics and analysis, I'll now build the final model taking into account our above recommendations\n\n#setup with my final model predictors in a list\nfinal_predictor_vars = ['sex', 'sbp', 'scl', 'age', 'bmi', 'spring', 'summer', 'fall']\n\n#show the final model predictors I've selected\nprint(\"Final model predictors:\")\nfor var in final_predictor_vars:\n    print(f\"  - {var}\")\n\n#prepare final model data\nfinal_outcome_var = framingham_complete_df['chdfate']\nfinal_predictor_data = framingham_complete_df[final_predictor_vars].copy()\nfinal_predictor_data_with_const = sm.add_constant(final_predictor_data)\n\n#show the suggested model\nprint(f\"✓ Final model data prepared\")\nprint(f\"  Observations: {len(final_outcome_var):,}\")\nprint(f\"  Predictors: {len(final_predictor_vars)}\")\n\n#fit final model\nfinal_logit_model = sm.Logit(final_outcome_var, final_predictor_data_with_const).fit()\n\n#show the final model\nprint(\"✓ Final logistic regression model fitted successfully\")\nprint(f\"  Convergence: {final_logit_model.mle_retvals['converged']}\")\nprint(f\"  Iterations: {final_logit_model.mle_retvals['iterations']}\")\n\n#final model summary\nprint(final_logit_model.summary())\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "589c225c-ab9e-43d7-a7b2-aca43819dacc",
   "metadata": {
    "name": "FinalModelPerformance",
    "collapsed": false
   },
   "source": "### Final Model Performance\n\n- **Pseudo R²**: 0.076 (7.65% variance explained)\n- **Log-Likelihood**: -2,675.8\n- **LLR p-value**: 1.167e-90 (highly significant)\n- **Convergence**: Successful in 6 iterations\n\nAll five cardiovascular risk factors remain highly significant (p < 0.001): sex, sbp, scl, \nage, and bmi. \n\nSeasonal variables are not significant."
  },
  {
   "cell_type": "markdown",
   "id": "81e6467b-ed0a-4384-95c8-3e747b2c68f3",
   "metadata": {
    "name": "Question9",
    "collapsed": false
   },
   "source": "## Question 9: Odds Ratios and Interpretation\n\nOdds ratios represent the multiplicative change in CHD odds for a one-unit increase in each \npredictor, holding all other variables constant.\n\n**Interpretation Guide:**\n- OR > 1: Increased odds of CHD\n- OR < 1: Decreased odds of CHD\n- OR = 1: No association with CHD"
  },
  {
   "cell_type": "code",
   "id": "843dc52d-e4e8-454b-90c4-46b689362206",
   "metadata": {
    "language": "python",
    "name": "CalculateOddsRatios"
   },
   "outputs": [],
   "source": "#setup odds calculations\nodds_ratios = np.exp(final_logit_model.params)\nconfidence_intervals = np.exp(final_logit_model.conf_int())\nconfidence_intervals.columns = ['CI_Lower', 'CI_Upper']\n\n#do the odds calcs and load into a dataframe just for this purpose\nodds_ratio_results_df = pd.DataFrame({\n    'Odds_Ratio': odds_ratios,\n    'CI_Lower_95': confidence_intervals['CI_Lower'],\n    'CI_Upper_95': confidence_intervals['CI_Upper'],\n    'P_value': final_logit_model.pvalues,\n    'Significant': final_logit_model.pvalues < 0.05\n}).round(4)\n\n#show\nodds_ratio_results_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d13b74e1-e3b9-4324-abc1-fc43c7343ba9",
   "metadata": {
    "name": "InterpretSignificantPredictors",
    "collapsed": false
   },
   "source": "### Interpretation of Significant Predictors\n\n#### Sex (Female vs Male)\n**OR = 0.447 (95% CI: 0.392 - 0.510), p < 0.001**\n\nFemales have approximately **55% lower odds** of developing CHD compared to males. This \nrepresents a strong protective effect consistent with known sex differences in evaluating cardiovascular \ndisease.\n\n#### Systolic Blood Pressure (SBP) (per 1 mmHg)\n**OR = 1.011 (95% CI: 1.007 - 1.014), p < 0.001**\n\nEach 1 mmHg increase in SBP increases CHD odds by 1.1%. More meaningfully, a **10 mmHg \nincrease** corresponds to an **11.6% increase** in CHD odds (1.011^10 = 1.116).\n\n#### Serum Cholesterol (per 1 mg/dL)\n**OR = 1.007 (95% CI: 1.005 - 1.008), p < 0.001**\n\nEach 1 mg/dL increase in cholesterol increases CHD odds by 0.7%. A **40 mg/dL increase** \n(e.g., 200 to 240 mg/dL) corresponds to a **32% increase** in CHD odds (1.007^40 = 1.32).\n\n#### Age (per 1 year)\n**OR = 1.018 (95% CI: 1.009 - 1.026), p < 0.001**\n\nEach additional year of age increases CHD odds by 1.8%. Over **10 years**, this translates \nto a **19.7% increase** in CHD odds (1.018^10 = 1.197).\n\n#### Body Mass Index (per 1 kg/m²)\n**OR = 1.048 (95% CI: 1.031 - 1.066), p < 0.001**\n\nEach 1-unit BMI increase raises CHD odds by 4.8%. A **5-unit BMI increase** (e.g., BMI 25 \nto 30) corresponds to a **26.5% increase** in CHD odds (1.048^5 = 1.265).\n\n#### Seasonal Variables\nSpring, summer, and fall showed no significant associations with CHD (all p > 0.3), \nsuggesting season of baseline examination does not meaningfully predict CHD risk.\n"
  },
  {
   "cell_type": "code",
   "id": "48fe9f5f-415d-4400-866b-41b7416d33bf",
   "metadata": {
    "language": "python",
    "name": "VisualizeOddsRatios"
   },
   "outputs": [],
   "source": "#setup data frames to chart\nplot_data_df = odds_ratio_results_df[odds_ratio_results_df.index != 'const'].copy()\nplot_data_df = plot_data_df.sort_values('Odds_Ratio')\n\n#build data frames to plot\nfig, ax = plt.subplots(figsize=(10, 8))\n\nfor i, (idx, row) in enumerate(plot_data_df.iterrows()):\n    color = 'green' if row['Significant'] else 'gray'\n    ax.errorbar(\n        row['Odds_Ratio'], i,\n        xerr=[[row['Odds_Ratio'] - row['CI_Lower_95']], \n              [row['CI_Upper_95'] - row['Odds_Ratio']]],\n        fmt='o', markersize=10, capsize=5, color=color, ecolor='black'\n    )\n\n#setup charts\nax.axvline(x=1, color='red', linestyle='--', linewidth=2, label='OR = 1 (No effect)')\nax.set_yticks(range(len(plot_data_df)))\nax.set_yticklabels(plot_data_df.index)\nax.set_xlabel('Odds Ratio', fontsize=12, fontweight='bold')\nax.set_title('Odds Ratios for CHD with 95% Confidence Intervals', \n             fontsize=14, fontweight='bold')\nax.set_xscale('log')\nax.legend()\nax.grid(True, alpha=0.3)\n\n#show our great work\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6a9516b-d998-4ea4-b44b-0a85bd9f2674",
   "metadata": {
    "name": "ModelPerformanceEvaluation",
    "collapsed": false
   },
   "source": "## Model Performance Evaluation\n\nAssess my final model's predictive performance using classification metrics and ROC \nanalysis."
  },
  {
   "cell_type": "code",
   "id": "7dfd01ef-e82f-46de-bf5b-d00706620d3d",
   "metadata": {
    "language": "python",
    "name": "Predictions"
   },
   "outputs": [],
   "source": "#generate predictions\npredicted_probabilities = final_logit_model.predict(final_predictor_data_with_const)\npredicted_classes = (predicted_probabilities > 0.5).astype(int)\n\n#show predictions\nprint(\"✓ Predictions generated\")\nprint(f\"  Prediction threshold: 0.5\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc75b768-4b89-43ad-b087-13de0f7235ea",
   "metadata": {
    "language": "python",
    "name": "ConfusionMatrix"
   },
   "outputs": [],
   "source": "#let's continue our assessment with my favorite view - the confusoin matrix!\n\n#create the confusion matrix dataframe\nconf_matrix = confusion_matrix(final_outcome_var, predicted_classes)\n\nconf_matrix_df = pd.DataFrame(\n    conf_matrix,\n    columns=['Predicted_No_CHD', 'Predicted_CHD'],\n    index=['Actual_No_CHD', 'Actual_CHD']\n)\n\n#show it\nconf_matrix_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9c4e878-2287-4d22-8e67-7ba7a4824800",
   "metadata": {
    "language": "python",
    "name": "ClassificationReport"
   },
   "outputs": [],
   "source": "#show the classification report\nprint(classification_report(\n    final_outcome_var, \n    predicted_classes, \n    target_names=['No CHD', 'CHD']\n))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67a5d8ae-8ee4-475f-8550-e7f5bac421f3",
   "metadata": {
    "language": "python",
    "name": "AUCandROC"
   },
   "outputs": [],
   "source": "#continue to evaluate my model by looking at AUC-ROC\n\n#calculate AUC-ROC\nauc_score = roc_auc_score(final_outcome_var, predicted_probabilities)\n\n#show it\nprint(f\"AUC-ROC Score: {auc_score:.4f}\")\nprint(\"\\nInterpretation:\")\nprint(\"  0.5-0.7: Poor discrimination\")\nprint(\"  0.7-0.8: Acceptable discrimination\")\nprint(\"  0.8-0.9: Excellent discrimination\")\nprint(\"  >0.9: Outstanding discrimination\")\n\n#ROC curve\nfpr, tpr, thresholds = roc_curve(final_outcome_var, predicted_probabilities)\n\n#plot curve\nfig, ax = plt.subplots(figsize=(8, 8))\nax.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\nax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate', fontsize=12)\nax.set_title('ROC Curve for CHD Prediction Model', fontsize=14, fontweight='bold')\nax.legend(fontsize=12)\nax.grid(True, alpha=0.3)\n\n#show curve\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9152187e-2a39-4766-8995-d5a547e70d7c",
   "metadata": {
    "name": "Conclusion",
    "collapsed": false
   },
   "source": "## Conclusions\n\n### Key Findings\n\n1. **Model Performance**: The final logistic regression model achieved moderate discrimination \n   (AUC-ROC = 0.689) and is highly statistically significant (p < 0.001).\n\n2. **Significant Risk Factors**: Five cardiovascular risk factors showed significant \n   associations with CHD:\n   - Female sex (protective)\n   - Systolic blood pressure\n   - Serum cholesterol\n   - Age\n   - Body mass index\n\n3. **Protective Effect of Female Sex**: Females demonstrated 55% lower CHD odds compared to \n   males, consistent with known sex differences.\n\n4. **Modifiable Risk Factors**: Blood pressure, cholesterol, and BMI represent important \n   modifiable targets for CHD prevention.\n\n5. **Model Diagnostics**: After addressing multicollinearity by removing DBP, all diagnostic \n   checks were satisfactory with no major violations of assumptions.\n\n### Limitations\n\n- **Model Fit**: Pseudo R² of 7.65% indicates substantial unexplained variation in CHD risk\n- **Missing Data**: Complete case analysis excluded 41 observations (0.87% loss)\n- **Binary Outcome**: Does not capture disease severity or time to event\n\n### Clinical Implications\n\nThe findings support the importance of traditional cardiovascular risk factor management. \nClinical interventions targeting blood pressure control, cholesterol reduction, and weight \nmanagement are likely to yield meaningful reductions in CHD risk. The strong protective \neffect of female sex suggests that understanding hormonal and biological mechanisms could \ninform prevention strategies.\n\n### What Might Improve My Model\n\nIt seems another potential indicator for CHD is hereditary indicators. I'd love to have \nthat data to include in our model and test if it's a more influential indicator, an\nenhancement to existing indicators or no influence at all. In my personal case, I note\nmy doctors, over the years, have keyed on my father's CHD in determining appropriate\ndiagnostic examinations for me through my life. This is the case, even though he remains\nfit and healthy at the age of 96!\n\n### FINAL CONCLUSION\n\nI'll be spending more time in the gym!!!"
  }
 ]
}