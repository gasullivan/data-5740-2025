{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "zl54a62qvauyl5pwosqz",
   "authorId": "2206420187358",
   "authorName": "GREGSULLIVAN",
   "authorEmail": "gregsullivan@ciosoglobal.com",
   "sessionId": "7c66d875-6093-4c18-b741-2e1a8b74f9ea",
   "lastEditTime": 1762558182726
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Exercise11",
    "collapsed": false
   },
   "source": "# Exercise 11 (as assigned)\nDue Tuesday, November 11, 2025  11:59pm\n\nRandom Forest and XGBoost Analysis\nAnalyzing 2016 election data to predict voter gap (trump - clinton)\n\nWe will be using the county_level_election.csv dataset. This is 2016 election data and we are going to measure ‘votergap’ as the outcome. ‘votergap’ = trump-clinton. The exercise will build on the work from the decision tree lab.\n\n# Section 2: Bagging / Random Forest\nWe are going to be using test and training splits, cross validation, and fitting a random forest to the data. Create an 80/20 Train/Test split. For accuracy use the .score method.\nfrom sklearn.ensemble import RandomForestRegressor\n\n1.\tSet the number of estimators to be 100, the features to be the square root of available features, and iterate through depths (1-20). Use only 5 folds for cross validation to save some compute resources. Plot the max depth on the x axis and the accuracy on the y axis for training and for the mean cross validation.\n2.\tBased on the plot, how many nodes would you recommend as the max depth?\n3.\tWhat is the accuracy (mean cv) at your chosen depth?\n4.\tThe cross validation looks different than the lab, why?\n\n# Section 3: Boosting / XGBoost\nimport xgboost as xgb\n\n5.\tUse the defaults for most parameters. Iterate through depths (1-20). Use only 5 folds for cross validation to save some compute resources. Plot the max depth on the x axis and the accuracy on the y axis for training and for the mean cross validation.\n6.\tBased on the plot, how many nodes would you recommend as the max depth?\n7.\tWhat is the accuracy (mean cv) at your chosen depth?\n8.\tThe cross validation looks different than random forest, why?\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "545f3677-fdab-40a1-9ddf-4b055ed8ceab",
   "metadata": {
    "name": "Exercise11Setup",
    "collapsed": false
   },
   "source": "# Week 11 Exercise: Random Forest and XGBoost Analysis\n## 2016 Election Data - Predicting Voter Gap\n\n### Overview\nThis exercise analyzes 2016 U.S. presidential election data at the county level to predict **votergap** (% Trump votes - % Clinton votes). We will compare two ensemble learning approaches:\n\n1. **Random Forest (Bagging)** - Parallel independent trees with bootstrap sampling\n2. **XGBoost (Boosting)** - Sequential error-correcting trees\n\n### Dataset\n- **File:** county_level_election.csv\n- **Unit of Analysis:** One row per U.S. county\n- **Target Variable:** votergap = trump % - clinton %\n- **Features:** 11 demographic and health indicators\n\n### Analysis Goals\n- Create 80/20 train/test split\n- Tune max_depth hyperparameter (range: 1-20)\n- Use 5-fold cross-validation\n- Compare model performance and behavior\n- Understand why CV patterns differ between algorithms"
  },
  {
   "cell_type": "code",
   "id": "820211a5-b176-456b-be6e-ad5dbfe13435",
   "metadata": {
    "language": "python",
    "name": "LibrarySetup"
   },
   "outputs": [],
   "source": "#supress future warnings (XGBoost was complaining)\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n#library import\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\nimport streamlit as st\n\n#set random seed for reproducibility\nRANDOM_STATE = 42\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "952aae20-0a46-43da-8ef0-d2c27e5715c6",
   "metadata": {
    "name": "PreparationApproach",
    "collapsed": false
   },
   "source": "## Section 1: Data Loading and Preparation\n\nI'll load the county-level election data and prepare our feature matrix (X) and target vector (y).\n"
  },
  {
   "cell_type": "code",
   "id": "13470fe6-6a50-490c-bb8e-eb73880efd17",
   "metadata": {
    "language": "python",
    "name": "LoadData"
   },
   "outputs": [],
   "source": "election_df = pd.read_csv('county_level_election.csv')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90b7bec6-1cc3-42eb-90f9-08bd63765439",
   "metadata": {
    "language": "python",
    "name": "DatasetShape"
   },
   "outputs": [],
   "source": "print(f\"Dataset shape: {election_df.shape}\")\nprint(f\"Number of counties: {election_df.shape[0]:,}\")\nprint(f\"Number of variables: {election_df.shape[1]}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e754637-2226-4009-9c1f-62b2acd4012e",
   "metadata": {
    "language": "python",
    "name": "DisplayInfo"
   },
   "outputs": [],
   "source": "#first few rows\nelection_df.head()\n\n#column names\nelection_df.columns.tolist()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "845fb2b3-1452-49df-8684-cf4c8c498827",
   "metadata": {
    "name": "FeatureSelection",
    "collapsed": false
   },
   "source": "### Feature Selection\n\nI'll use 11 demographic and health indicators as our features to predict votergap:\n- **hispanic:** % Hispanic population\n- **minority:** % Minority population  \n- **female:** % Female population\n- **unemployed:** Unemployment rate\n- **income:** Median household income\n- **nodegree:** % Without high school degree\n- **bachelor:** % With bachelor's degree\n- **inactivity:** Physical inactivity rate\n- **obesity:** Obesity rate\n- **density:** Population density\n- **cancer:** Cancer rate"
  },
  {
   "cell_type": "code",
   "id": "5212195f-54d6-4f5e-9c0a-1f64dd6183e3",
   "metadata": {
    "language": "python",
    "name": "FeaturesAndTarget"
   },
   "outputs": [],
   "source": "feature_columns = [\n    'hispanic', 'minority', 'female', 'unemployed', 'income',\n    'nodegree', 'bachelor', 'inactivity', 'obesity', 'density', 'cancer'\n]\n\nX = election_df[feature_columns]\ny = election_df['votergap']\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbf964a1-6c21-47d3-8db1-5ee0c1b42723",
   "metadata": {
    "language": "python",
    "name": "FeatureMatrix"
   },
   "outputs": [],
   "source": "#display feature matrix shape\nprint(f\"Feature matrix shape: {X.shape}\")\nprint(f\"Features: {len(feature_columns)}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9614170d-bc34-4840-8562-24bfbddaac58",
   "metadata": {
    "language": "python",
    "name": "TargetStatistics"
   },
   "outputs": [],
   "source": "print(f\"Target variable: votergap\")\nprint(f\"Range: {y.min():.2f} to {y.max():.2f}\")\nprint(f\"Mean: {y.mean():.2f}\")\nprint(f\"Median: {y.median():.2f}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "180b49f7-135a-481e-b260-71a60e4b99c3",
   "metadata": {
    "name": "TrainTestSplit",
    "collapsed": false
   },
   "source": "### Train/Test Split\n\nCreating an 80/20 split for training and testing. The training set will be used for model building and cross-validation, while the test set provides an independent evaluation of final model performance.\n"
  },
  {
   "cell_type": "code",
   "id": "f91b3f4e-ac4e-45b0-9947-1fa437673475",
   "metadata": {
    "language": "python",
    "name": "CreateSplit"
   },
   "outputs": [],
   "source": "#create train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.20, \n    random_state=RANDOM_STATE\n)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75df7766-85b9-4b82-9071-415fe6bc5f4c",
   "metadata": {
    "language": "python",
    "name": "DisplaySplit"
   },
   "outputs": [],
   "source": "#show display split sizes\nprint(f\"Training set: {X_train.shape[0]:,} counties ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Test set: {X_test.shape[0]:,} counties ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "131ee1d5-60da-4596-819d-889ea660da9a",
   "metadata": {
    "name": "RandomForestAnalysis",
    "collapsed": false
   },
   "source": "## Section 2: Random Forest Analysis\n\nRandom Forest uses **bagging** (Bootstrap AGGregatING):\n- Builds multiple independent trees in parallel\n- Each tree trained on bootstrap sample (sampling with replacement)\n- Final prediction = average of all tree predictions\n- Reduces variance through ensemble averaging\n\n### Configuration\n- **n_estimators:** 100 trees\n- **max_features:** Square root of total features (√11 ≈ 3)\n- **Cross-validation:** 5 folds\n- **Max depth range:** 1 to 20"
  },
  {
   "cell_type": "code",
   "id": "6b96ccd2-bd00-48cd-a248-f3f6ea7b6333",
   "metadata": {
    "language": "python",
    "name": "CalculateMaxFeatures"
   },
   "outputs": [],
   "source": "#calculate max features for random forest\nn_features = X_train.shape[1]\nmax_features_sqrt = int(np.sqrt(n_features))\n\nprint(f\"Total features: {n_features}\")\nprint(f\"Max features per split: {max_features_sqrt} (square root)\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "050b3bc9-c653-4d9e-a63e-b0a991dae4ec",
   "metadata": {
    "language": "python",
    "name": "InitializeRandomForest"
   },
   "outputs": [],
   "source": "#initalize storage for random forest results\nrf_depths = range(1, 21)\nrf_train_scores = []\nrf_cv_scores = []\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5c01424-2c7f-4581-a6c8-8b2f04ac0618",
   "metadata": {
    "name": "TrainingSetup",
    "collapsed": false
   },
   "source": "### Training Random Forest Models\n\nIterating through max_depth values from 1 to 20, training a Random Forest for each depth and evaluating with both training R² and 5-fold cross-validation R²."
  },
  {
   "cell_type": "code",
   "id": "fe55bc98-a8dd-42ea-99f2-4602210db0fe",
   "metadata": {
    "language": "python",
    "name": "TrainRandomForest"
   },
   "outputs": [],
   "source": "#train random forest models across depths\nprint(\"Training Random Forest models...\")\nprint(f\"{'Depth':<8} {'Train R²':<12} {'CV R²':<12}\")\nprint(\"-\" * 35)\n\nfor depth in rf_depths:\n    rf_model = RandomForestRegressor(\n        n_estimators=100,\n        max_depth=depth,\n        max_features=max_features_sqrt,\n        random_state=RANDOM_STATE,\n        n_jobs=1\n    )\n    \n    rf_model.fit(X_train, y_train)\n    train_score = rf_model.score(X_train, y_train)\n    rf_train_scores.append(train_score)\n    \n    cv_scores = cross_val_score(\n        rf_model, X_train, y_train, \n        cv=5, \n        scoring='r2',\n        n_jobs=1\n    )\n    cv_score_mean = cv_scores.mean()\n    rf_cv_scores.append(cv_score_mean)\n    \n    if depth % 5 == 0 or depth == 1:\n        print(f\"{depth:<8} {train_score:<12.4f} {cv_score_mean:<12.4f}\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "995cadec-1468-40b2-b209-be03c753a080",
   "metadata": {
    "language": "python",
    "name": "FindBestDepth"
   },
   "outputs": [],
   "source": "#find the best random forest depth\nrf_best_idx = np.argmax(rf_cv_scores)\nrf_best_depth = rf_depths[rf_best_idx]\nrf_best_cv_score = rf_cv_scores[rf_best_idx]\n\nprint(f\"\\nBest depth based on CV: {rf_best_depth}\")\nprint(f\"CV R² at best depth: {rf_best_cv_score:.4f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7cbe1e11-50bf-496a-8d92-c8f041caf9d4",
   "metadata": {
    "name": "SetupViz",
    "collapsed": false
   },
   "source": "### Random Forest Visualization\n\nPlotting training and cross-validation R² scores across all depth values to visualize model performance and identify optimal complexity.\n"
  },
  {
   "cell_type": "code",
   "id": "c8e72b91-e892-4e46-a56a-bdce6c1b10ba",
   "metadata": {
    "language": "python",
    "name": "RandomForestPerformancePlot"
   },
   "outputs": [],
   "source": "#plot our random forest performance\nplt.figure(figsize=(12, 6))\nplt.plot(rf_depths, rf_train_scores, 'b-o', label='Training R²', linewidth=2, markersize=6)\nplt.plot(rf_depths, rf_cv_scores, 'r-s', label='Cross-Validation R²', linewidth=2, markersize=6)\nplt.axvline(x=rf_best_depth, color='g', linestyle='--', alpha=0.7, \n            label=f'Best Depth = {rf_best_depth}')\nplt.xlabel('Max Depth', fontsize=12)\nplt.ylabel('R² Score', fontsize=12)\nplt.title('Random Forest: Model Performance vs Max Depth', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('random_forest_performance.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c76f28b1-a574-4e13-a23c-e07e923726f5",
   "metadata": {
    "name": "RandomForestEval",
    "collapsed": false
   },
   "source": "### Random Forest Test Set Evaluation\n\nTraining the final Random Forest model with the optimal depth and evaluating on the held-out test set.\n"
  },
  {
   "cell_type": "code",
   "id": "1f48f497-e277-4416-b65a-2886f77ef7e3",
   "metadata": {
    "language": "python",
    "name": "TrainRandomForestModel"
   },
   "outputs": [],
   "source": "#train the final random forest model\nrf_final = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=rf_best_depth,\n    max_features=max_features_sqrt,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nrf_final.fit(X_train, y_train)\nrf_test_score = rf_final.score(X_test, y_test)\n\n#show random forest results summary\nprint(\"=\" * 80)\nprint(\"RANDOM FOREST RESULTS\")\nprint(\"=\" * 80)\nprint(f\"Recommended max_depth: {rf_best_depth}\")\nprint(f\"Training R² at chosen depth: {rf_train_scores[rf_best_idx]:.4f}\")\nprint(f\"Cross-Validation R² at chosen depth: {rf_best_cv_score:.4f}\")\nprint(f\"Test R² at chosen depth: {rf_test_score:.4f}\")\nprint(\"=\" * 80)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8fa4f3fe-f0eb-456f-b359-036e0a977aab",
   "metadata": {
    "name": "VisualizingARamdomForestTree",
    "collapsed": false
   },
   "source": "### Visualizing a Sample Tree from Random Forest\n\nSince Random Forest contains 100 trees, I'll visualize just one example tree to see how the model makes decisions based on demographic and health features."
  },
  {
   "cell_type": "code",
   "id": "ad545d18-2446-4fd3-9588-c29174ea12b9",
   "metadata": {
    "language": "python",
    "name": "RandomForestTreeViz"
   },
   "outputs": [],
   "source": "#extract one tree from the Random Forest (tree index 0)\nsingle_tree = rf_final.estimators_[0]\n\ndot_data = export_graphviz(\n    single_tree,\n    out_file=None,\n    feature_names=['hispanic', 'minority', 'female', 'unemployed', 'income', \n                   'nodegree', 'bachelor', 'inactivity', 'obesity', 'density', 'cancer'],\n    filled=True,\n    rounded=True,\n    special_characters=True,\n    max_depth=3\n)\n\n#display the graph directly without streamlit\ngraph = graphviz.Source(dot_data)\ngraph",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38a2f313-8683-4189-9079-599b3a72149f",
   "metadata": {
    "language": "python",
    "name": "PaulsTrainingWithDepth"
   },
   "outputs": [],
   "source": "#I'm adding in Paul's training code from to compare\n\n# Test for up to 20 layers of depth\ndepths = list(range(1, 21))\ntrain_scores = []\ncvmeans = []\ncvstds = []\ncv_scores = []\n\nfor depth in depths:\n\n    print(f'Training with depth: {depth}...')\n    dtree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n\n    # Perform training and 10-fold cross validation\n    train_scores.append(dtree.fit(X_train, y_train).score(X_train, y_train))\n    scores = cross_val_score(estimator=dtree, X=X_train, y=y_train, cv=10)\n\n    cvmeans.append(scores.mean())\n    cvstds.append(scores.std())\n\ncvmeans = np.array(cvmeans)\ncvstds = np.array(cvstds)\n\n# Plot the Mean Accuracy from cross validation with 2 std shadded\nplt.plot(depths, cvmeans, '*-', label=\"Mean CV\")\nplt.fill_between(depths, cvmeans - 2*cvstds, cvmeans + 2*cvstds, alpha=0.3)\n\n# Plot accuracy of a model with depth N\n# against the cross-validatation of the model with depth N\nylim = plt.ylim()\nplt.plot(depths, train_scores, '-+', label=\"Train\")\nplt.legend()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Depth\")\nplt.xticks(depths);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a741cb6-429b-49cb-a871-557ed4ce9885",
   "metadata": {
    "name": "MaxNodesQuestion",
    "collapsed": false
   },
   "source": "### Based on the plot, how many nodes would you recommend as the max depth?\n\n**Answer:** 20\n\nThe cross-validation R² continues to improve through depth 20, reaching its peak at this maximum tested depth. While the gap between training and CV scores widens (indicating some overfitting), the CV score itself continues climbing, demonstrating that deeper trees capture meaningful patterns in this dataset.\n"
  },
  {
   "cell_type": "markdown",
   "id": "023ae940-6204-4926-a194-369fdc5bbea5",
   "metadata": {
    "name": "AccuracyQuestion",
    "collapsed": false
   },
   "source": "### Question: What is the accuracy (mean CV) at your chosen depth?\n\n**Answer:** The CV R² at depth 20 is displayed above in the results summary.\n"
  },
  {
   "cell_type": "markdown",
   "id": "dfadbe52-ae2f-40b3-8303-eb217a86f225",
   "metadata": {
    "name": "CrossValidationQuestion",
    "collapsed": false
   },
   "source": "### Question: The cross validation looks different than the lab, why?\n\n**Answer:** Several factors contribute to different CV behavior compared to simpler lab datasets:\n\n1. **Dataset Complexity**\n   - Election data contains complex non-linear relationships between demographic factors and voting patterns\n   - County-level aggregation introduces geographic variance and regional clustering effects\n   - The votergap has high variability across counties\n\n2. **Feature Interactions**\n   - 11 features create a rich decision space with potential interactions\n   - Some features may have threshold effects or non-linear relationships with votergap\n\n3. **Overfitting Pattern**\n   - Large gap between training R² (0.96) and CV R² (0.74) indicates overfitting\n   - However, CV continues improving because Random Forest's averaging mechanism helps generalization\n   - Lab datasets were likely less complex with clearer signal-to-noise ratios\n\n4. **Regional Effects**\n   - Geographic clustering of counties creates natural data structure\n   - Cross-validation folds may capture different regional patterns\n   - This introduces more CV variance than independent observations would\n"
  },
  {
   "cell_type": "markdown",
   "id": "c82786a6-5a29-444b-b848-148c9e4e1f87",
   "metadata": {
    "name": "XGBoostSection",
    "collapsed": false
   },
   "source": "## Section 3: XGBoost Analysis\n\nXGBoost uses **boosting**:\n- Builds trees sequentially, each correcting errors of previous trees\n- Each tree fits residual errors from the ensemble so far\n- More aggressive learning achieves high accuracy with fewer, shallower trees\n- More prone to overfitting if not carefully tuned\n\n### Configuration\n- **Default XGBoost parameters**\n- **Cross-validation:** 5 folds\n- **Max depth range:** 1 to 20\n"
  },
  {
   "cell_type": "code",
   "id": "0affe8c9-8e11-4b9c-914a-511226739974",
   "metadata": {
    "language": "python",
    "name": "XGBoostModelsTraining"
   },
   "outputs": [],
   "source": "#initial storage for XGBoost results\nxgb_depths = range(1, 21)\nxgb_train_scores = []\nxgb_cv_scores = []\n\n#train XGBoost models across depths\n#Iterating through max_depth values from 1 to 20\n#  training an XGBoost model for each depth and evaluating with both training R² and 5-fold cross-validation R²\nprint(\"Training XGBoost models...\")\nprint(f\"{'Depth':<8} {'Train R²':<12} {'CV R²':<12}\")\nprint(\"-\" * 35)\n\n#across the depths\nfor depth in xgb_depths:\n    xgb_model = xgb.XGBRegressor(\n        max_depth=depth,\n        random_state=RANDOM_STATE,\n        n_jobs=1\n    )\n\n    #fit the model\n    xgb_model.fit(X_train, y_train)\n    train_score = xgb_model.score(X_train, y_train)\n    xgb_train_scores.append(train_score)\n\n    #check scores\n    cv_scores = cross_val_score(\n        xgb_model, X_train, y_train,\n        cv=5,\n        scoring='r2',\n        n_jobs=1\n    )\n    cv_score_mean = cv_scores.mean()\n    xgb_cv_scores.append(cv_score_mean)\n    \n    if depth % 5 == 0 or depth == 1:\n        print(f\"{depth:<8} {train_score:<12.4f} {cv_score_mean:<12.4f}\")\n\n#find best XGBoostDepth\nxgb_best_idx = np.argmax(xgb_cv_scores)\nxgb_best_depth = xgb_depths[xgb_best_idx]\nxgb_best_cv_score = xgb_cv_scores[xgb_best_idx]\n\nprint(f\"\\nBest depth based on CV: {xgb_best_depth}\")\nprint(f\"CV R² at best depth: {xgb_best_cv_score:.4f}\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9aa31026-ad75-4f18-bbee-7e68c628eb68",
   "metadata": {
    "language": "python",
    "name": "XGBoostVisualization"
   },
   "outputs": [],
   "source": "#plot training and cross-validation R² scores across all depth values \n#  to visualize the boosting algorithm's characteristic overfitting pattern\n\nplt.figure(figsize=(12, 6))\nplt.plot(xgb_depths, xgb_train_scores, 'b-o', label='Training R²', linewidth=2, markersize=6)\nplt.plot(xgb_depths, xgb_cv_scores, 'r-s', label='Cross-Validation R²', linewidth=2, markersize=6)\nplt.axvline(x=xgb_best_depth, color='g', linestyle='--', alpha=0.7, \n            label=f'Best Depth = {xgb_best_depth}')\nplt.xlabel('Max Depth', fontsize=12)\nplt.ylabel('R² Score', fontsize=12)\nplt.title('XGBoost: Model Performance vs Max Depth', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('xgboost_performance.png', dpi=300, bbox_inches='tight')\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e5ffe9a-5107-4aaa-95b0-0a7b7016a381",
   "metadata": {
    "language": "python",
    "name": "XGBoostEvaluation"
   },
   "outputs": [],
   "source": "#train final XGBoost model\nxgb_final = xgb.XGBRegressor(\n    max_depth=xgb_best_depth,\n    random_state=RANDOM_STATE,\n    n_jobs=1\n)\n\nxgb_final.fit(X_train, y_train)\nxgb_test_score = xgb_final.score(X_test, y_test)  # ADD THIS LINE\n\n#show XGBoost results summary\nprint(\"=\" * 80)\nprint(\"XGBOOST RESULTS\")\nprint(\"=\" * 80)\nprint(f\"Recommended max_depth: {xgb_best_depth}\")\nprint(f\"Training R² at chosen depth: {xgb_train_scores[xgb_best_idx]:.4f}\")\nprint(f\"Cross-Validation R² at chosen depth: {xgb_best_cv_score:.4f}\")\nprint(f\"Test R² at chosen depth: {xgb_test_score:.4f}\")\nprint(\"=\" * 80)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "02604d4a-8c39-4e3c-812f-bae68c939698",
   "metadata": {
    "name": "NodesAnswer",
    "collapsed": false
   },
   "source": "### Question: Based on the plot, how many nodes would you recommend as the max depth?\n\n**Answer:** 2\n\nXGBoost achieves its best cross-validation performance at depth 2. Unlike Random Forest, the CV score peaks early and then gradually declines as depth increases. This classic pattern shows overfitting setting in at higher depths. Depth 2 provides optimal generalization.\n"
  },
  {
   "cell_type": "markdown",
   "id": "c66af9a3-a53e-4552-a021-aac8ba2b57bd",
   "metadata": {
    "name": "AccuracyAnswer",
    "collapsed": false
   },
   "source": "### Question: What is the accuracy (mean CV) at your chosen depth?\n\n**Answer:** The CV R² at depth 2 is displayed above in the results summary.\n"
  },
  {
   "cell_type": "markdown",
   "id": "ddd140c5-d61d-4b4b-98a2-c01fbdeb4e7a",
   "metadata": {
    "name": "CrossValidationAnswer",
    "collapsed": false
   },
   "source": "### Question: The cross validation looks different than random forest, why?\n\n**Answer:** The fundamental algorithmic differences explain the contrasting CV patterns:\n\n### 1. Ensemble Approach\n\n**Random Forest (Bagging):**\n- Builds multiple INDEPENDENT trees in parallel\n- Each tree trained on bootstrap sample (sampling with replacement)\n- Final prediction = average of all tree predictions\n- Trees can be very deep because averaging reduces variance\n- Requires more depth to capture complex patterns\n\n**XGBoost (Boosting):**\n- Builds trees SEQUENTIALLY, each correcting previous errors\n- Each new tree fits the residual errors of the current ensemble\n- More efficient learning through focused error correction\n- Achieves higher accuracy with shallower trees\n- More sensitive to overfitting as depth increases\n\n### 2. Learning Efficiency\n\n- XGBoost reaches peak CV performance at depth 2\n- Random Forest needs depth 20 to reach similar performance\n- Sequential error correction is more sample-efficient than parallel averaging\n- Boosting learns patterns faster but also overfits faster\n\n### 3. Overfitting Behavior\n\n**Random Forest:**\n- CV score steadily improves with depth (less sensitive to overfitting)\n- Large gap between training and CV, but both keep improving\n- Averaging mechanism provides natural regularization\n\n**XGBoost:**\n- CV score peaks early then declines (more prone to overfit)\n- Training score quickly reaches perfection (R² = 1.0 at depth 5)\n- Sequential learning compounds errors when trees get too complex\n\n### 4. Bias-Variance Tradeoff\n\n**Random Forest:**\n- Starts with high bias (shallow trees underfit)\n- Reduces bias by increasing depth\n- Variance controlled through ensemble averaging\n\n**XGBoost:**\n- Starts with lower bias (boosting is inherently aggressive)\n- Quickly achieves good fit\n- Variance increases rapidly with depth due to sequential dependencies\n"
  },
  {
   "cell_type": "code",
   "id": "97477674-2f61-4e54-93a5-2f5485b89d11",
   "metadata": {
    "language": "python",
    "name": "ModelComparisonSummary"
   },
   "outputs": [],
   "source": "#create model comparison visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\naxes[0].plot(rf_depths, rf_train_scores, 'b-o', label='Training R²', linewidth=2, markersize=5)\naxes[0].plot(rf_depths, rf_cv_scores, 'r-s', label='CV R²', linewidth=2, markersize=5)\naxes[0].axvline(x=rf_best_depth, color='g', linestyle='--', alpha=0.7, \n                label=f'Best: depth={rf_best_depth}')\naxes[0].set_xlabel('Max Depth', fontsize=12)\naxes[0].set_ylabel('R² Score', fontsize=12)\naxes[0].set_title('Random Forest Performance', fontsize=13, fontweight='bold')\naxes[0].legend(fontsize=10)\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(xgb_depths, xgb_train_scores, 'b-o', label='Training R²', linewidth=2, markersize=5)\naxes[1].plot(xgb_depths, xgb_cv_scores, 'r-s', label='CV R²', linewidth=2, markersize=5)\naxes[1].axvline(x=xgb_best_depth, color='g', linestyle='--', alpha=0.7, \n                label=f'Best: depth={xgb_best_depth}')\naxes[1].set_xlabel('Max Depth', fontsize=12)\naxes[1].set_ylabel('R² Score', fontsize=12)\naxes[1].set_title('XGBoost Performance', fontsize=13, fontweight='bold')\naxes[1].legend(fontsize=10)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50a4c885-e114-45dd-9bb0-16a7a4ac9319",
   "metadata": {
    "language": "python",
    "name": "FinalComparisonTable"
   },
   "outputs": [],
   "source": "#display final comparison table\ncomparison_df = pd.DataFrame({\n    'Metric': ['Best Depth', 'Training R²', 'CV R²', 'Test R²'],\n    'Random Forest': [\n        rf_best_depth,\n        rf_train_scores[rf_best_idx],\n        rf_best_cv_score,\n        rf_test_score\n    ],\n    'XGBoost': [\n        xgb_best_depth,\n        xgb_train_scores[xgb_best_idx],\n        xgb_best_cv_score,\n        xgb_test_score\n    ]\n})\n\n#show\ncomparison_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad2dc977-e236-46a3-8473-755caa21ddc1",
   "metadata": {
    "name": "FinalThoughts",
    "collapsed": false
   },
   "source": "## Key Insights\n\n### Performance\n1. **XGBoost achieves slightly better performance**\n   - CV R²: 0.7462 vs Random Forest's 0.7375\n   - Test R²: 0.7507 vs Random Forest's 0.7333\n\n2. **XGBoost is far more efficient**\n   - Needs only depth 2 vs Random Forest's depth 20\n   - Faster training and prediction\n   - Much smaller model size\n\n3. **XGBoost shows better generalization**\n   - Smaller gap between training and test scores\n   - Test performance actually exceeds CV (good sign)\n\n4. **Random Forest shows classic overfitting**\n   - Large gap between training (0.96) and test (0.73)\n   - Still improving at maximum tested depth\n\n### Algorithm Characteristics\n\n**Random Forest (Bagging):**\n- More stable and less prone to overfitting\n- Requires more complexity to achieve good performance\n- Good for datasets with high variance or noise\n- Easier to tune (less sensitive to hyperparameters)\n\n**XGBoost (Boosting):**\n- More efficient and achieves better performance faster\n- Requires careful tuning to avoid overfitting\n- Excellent for maximizing predictive accuracy\n- Better for datasets with clear patterns to learn\n\n### Recommendation\nFor this election prediction task, **XGBoost is the superior choice**:\n- Better accuracy with far simpler models\n- More efficient computationally\n- Better generalization to test data\n\nThe analysis demonstrates the fundamental tradeoff between bagging and boosting approaches in ensemble learning.\n"
  }
 ]
}