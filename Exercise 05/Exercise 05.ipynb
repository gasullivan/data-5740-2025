{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "3twm7zcd2gd52r2gut67",
   "authorId": "1043139642183",
   "authorName": "GASULLIVAN",
   "authorEmail": "sullivangregorya@wustl.edu",
   "sessionId": "da2008b0-9437-4aa3-9c1a-8ec8449eb411",
   "lastEditTime": 1759185258501
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Section1Description",
    "collapsed": false
   },
   "source": "Section 1: Multiple Regression\nA healthcare non-profit is interested in understanding the impact of statewide demographic information and cigarette prices on cigarette sales. They feel that if any of these factors are significantly related to cigarette sales, it will help them figure out which areas should be targeted with anti-smoking messaging. They provided you with the cigarette_sales.csv dataset. Information about this data is in the table below.\nVariable\tDescription\nAge\tMedian age of persons living in a state\nHS\t% of people >25 years of age in a state who had completed high school\nIncome\tPer capita personal income for a state in dollars\nBlack\t% of black race living in a state\nFemale\t% of females living in a state\nPrice\tWeighted average price in cents of a pack of cigarettes in a state\nSales\tNumber of packs of cigarettes sold in a state per person\n\nThis link will be helpful for doing this analysis. However, as this is multiple regression, you will need to make sure the X array contains all the predictor variables.\n1)\tAnswer the following:\na)\tWhat is the outcome? \nb)\tWhat are the predictors they want to understand the impact of? \nc)\tWhat is the hypothesis?\n2)\tExploratory data analysis\na)\tLook at a few rows of the data to understand how it is structured.\nb)\tGenerate some summary statistics using describe().\nc)\tLook at the distributions and scatterplots of the data. A convenient function for doing this is pairplot() in Seaborn. \nd)\tDo any of these variables look like they might violate the normality regression assumption or be correlated with other variables? Explain.\ne)\tGenerate a plot to check if there are outliers in the outcome. What do you see?\n3)\tMultiple regression\na)\tConduct a multiple regression analysis.\nb)\tAre any of the variables significant? \nc)\tInterpret the intercept and any statistically significant coefficients (i.e. what is their meaning in relation to sales?)\n"
  },
  {
   "cell_type": "markdown",
   "id": "8117c5ab-d31a-464a-9a54-77e7b34d8835",
   "metadata": {
    "name": "Q1abc",
    "collapsed": false
   },
   "source": "Answer the following:\n\n1(a)\tWhat is the outcome? \n\nThe outcome is sales (dependent variable).  We're trying to understand if any of the other variables can help us to predict sales.  If so, we can target anti-smoking campaigns more effectively.\n\n1(b)\n\nWhat are the potential predictors they want to understand the impact of?\n\nThe predictors are the other (independent) variables.  These are Age (Median age of people living in the state), HS (% people over age 25 with a high school diploma), Income (Per capita personal income), Black (% population identifying as Black), Female (% population identifying as Female), and Price (of a pack of cigarettes expressed in cents).\n\n1(c)\tWhat is the hypothesis?\n\nThe hypothesis is that the demographic data (age, high school, income, black and female), and cigarette prices are significantly related to cigarette sales.\n"
  },
  {
   "cell_type": "code",
   "id": "bbc00804-aa52-46d5-a5ea-2fec28420375",
   "metadata": {
    "language": "python",
    "name": "LoadCigCSVtoPandasDF"
   },
   "outputs": [],
   "source": "#2)\tExploratory data analysis\n# a)Look at a few rows of the data to understand how it's structured.\n# b)Generate some summary statistics using describe().\n\nimport pandas as pd\n\n#load the cigarette sales CSV into a pandas DataFrame\n\nsmokes_df = pd.read_csv('cigarette_sales.csv')\n\n#review data\n#2(a) first few rows\nprint(\"First 5 rows:\\n\", smokes_df.head())\n#2(b) summary stats\nprint(\"\\nSummary statistics (rounded):\\n\", smokes_df.describe().round(1))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fed3e61c-aff4-4192-81af-3847c640e817",
   "metadata": {
    "language": "python",
    "name": "PairPlot"
   },
   "outputs": [],
   "source": "#2(c). Look at the distributions and scatterplots of the data. \n# A convenient function for doing this is pairplot() in Seaborn. \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#2c distributions and scatter plots of data against all others\nsns.pairplot(smokes_df)\nplt.suptitle(\"Pairplot of Demographic Features and Cigarette Sales\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3be9281a-dadd-4a9f-b30a-ac37e949b05f",
   "metadata": {
    "language": "python",
    "name": "Q2Skew"
   },
   "outputs": [],
   "source": "#check for skew in our data\n\nsmokes_df.skew(numeric_only=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0935679-3d37-4421-96ff-3e55f2315a88",
   "metadata": {
    "language": "python",
    "name": "Q2CorrelationMatrix"
   },
   "outputs": [],
   "source": "#inspect the stability of the regression coefficients\nplt.figure(figsize=(10, 6))\nsns.heatmap(smokes_df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".1f\")\nplt.title(\"Correlation Matrix\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9597787b-ccec-41dc-a534-0627fc135260",
   "metadata": {
    "name": "Q2d",
    "collapsed": false
   },
   "source": "2(d)\tDo any of these variables look like they might violate the normality regression assumption or be correlated with other variables? Explain.\n\nHS (education rate) and Income may show moderate positive correlation (which makes sense).\n\nFemale and Age may have mild correlation.\n\nIncome and possibly Sales may violate the normality assumption due to skewness.\n\nThere is some moderate correlation between predictors, especially between Income and HS.\n\nNo immediate violations appear critical, further analysis could be conducted."
  },
  {
   "cell_type": "code",
   "id": "db71094c-eec3-4363-8407-6c56140375f8",
   "metadata": {
    "language": "python",
    "name": "Q2ePlot",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#2(e)Generate a plot to check if there are outliers in the outcome. What do you see?\n\n#boxplot to check for outliers in Sales\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=smokes_df['Sales'], color='skyblue')\nplt.title('Boxplot of Cigarette Sales per Capita')\nplt.xlabel('Sales')\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b1f2380-e1cd-4ef1-86f1-dba77456d3d8",
   "metadata": {
    "name": "Q2eWhatDoYouSee",
    "collapsed": false
   },
   "source": "By no surprise, we see states that are outliers - mostly on the high side (of sales).  This could represent differences in culture or regulation at the state level.  This could influence regression results.  As such, we may want to consider transformations of certain variables.  Skewed variables like Income and Sales may benefit from a log transformation."
  },
  {
   "cell_type": "markdown",
   "id": "dbe76ece-72fc-4db4-8197-7046f9e5e993",
   "metadata": {
    "name": "Section2Description",
    "collapsed": false
   },
   "source": "Section 2: Detecting Assumption Violations\nUsing the same data set and regression results from the prior section, do the following:\n1)\tCollinearity\na)\tCompute the VIF for each covariate and explain what the results mean. Use this link.\nb)\tCompute all the pairwise correlations between the variables. This link shows 3 ways to do this. Just use corr().\nc)\tRemove the 3 variables with the highest p-values. Refit the model. How have the p-values for the other variables changed? Did R2 change by much?\n2)\tModel Fit\na)\tWhat does R2 tell you about the fit of the second model?\nb)\tAs noted in the video on MLE, AIC is another measure of fit. Which model has the lowest AIC value (lowest is best)?\n3)\tOutliers\na)\tDo a leverage plot. Are there influential outliers? Again, this resource is helpful.\n4)\tLinearity & constant variance\na)\tGenerate a predicted vs standardized residual plot. The resid_studentized_internal in statsmodels are the standardized residuals. The link above shows how to obtain them. Does the data meet the linearity assumption?\n5)\tNormality\na)\tDo a Q-Q Plot of the residuals. Are the residuals normally distributed?\n\n"
  },
  {
   "cell_type": "code",
   "id": "388960e6-7908-4f3c-a780-6d65b9d69d17",
   "metadata": {
    "language": "python",
    "name": "Section2Q1aVIF"
   },
   "outputs": [],
   "source": "from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n#predictor variables\nX = smokes_df[['Age', 'HS', 'Income', 'Black', 'Female', 'Price']]\n\n#add a constant term for the intercept\nX = add_constant(X)\n\n#another DataFrame to hold VIF values\nvif_data = pd.DataFrame()\nvif_data['Variable'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n#show the VIF table\nprint(vif_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf57b23d-54a5-4ca8-bac6-bf1c8116cf4d",
   "metadata": {
    "language": "python",
    "name": "Section2Q1bPairwise"
   },
   "outputs": [],
   "source": "#compute pairwise correlations\ncorrelation_matrix = smokes_df.corr(numeric_only=True)\n\n#show the matrix\nprint(\"Pairwise Correlations:\\n\")\nprint(correlation_matrix.round(2))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b867cc5f-ea8e-4416-bc12-2ab9fe82e4a2",
   "metadata": {
    "language": "python",
    "name": "SectionQ21bHeatMap"
   },
   "outputs": [],
   "source": "#pairwise heatmap (much easier to read)\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation Matrix Heatmap')\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69a70721-e1a7-44a0-86bf-e0099b976393",
   "metadata": {
    "language": "python",
    "name": "Section2Q1cNoHiPvalue"
   },
   "outputs": [],
   "source": "#c)\tRemove the 3 variables with the highest p-values. Refit the model\n#   How have the p-values for the other variables changed? Did R2 change by much?\n\nimport statsmodels.api as sm\n\n#define outcome and predictors\nX = smokes_df[['Age', 'HS', 'Income', 'Black', 'Female', 'Price']]\ny = smokes_df['Sales']\n\n#add constant\nX = sm.add_constant(X)\n\n#initial model\nmodel_full = sm.OLS(y, X).fit()\nprint(\"Initial Model Summary:\")\nprint(model_full.summary())\n\n#find the 3 variables with highest p-values (excluding the constant)\npvals = model_full.pvalues.drop('const')\nhighest_pvars = pvals.sort_values(ascending=False).head(3).index.tolist()\n\n#drop those 3 predictors\nX_reduced = X.drop(columns=highest_pvars)\n\n#refit model\nmodel_reduced = sm.OLS(y, X_reduced).fit()\nprint(\"\\nRefit Model After Removing Highest P-Value Predictors:\")\nprint(model_reduced.summary())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19368179-33dc-4c0d-bf6d-f5463c72c961",
   "metadata": {
    "name": "Section2Q1cAdjustedModel",
    "collapsed": false
   },
   "source": "R² dropped slightly: from 0.321 (full model) to 0.303 (reduced model), which means the reduced model explains about 2% less of the variance in cigarette sales (which is acceptable)\n\nAdjusted R² increasedfrom 0.228 (full model) to 0.259 (reduced model), suggesting the reduced model is more efficient"
  },
  {
   "cell_type": "code",
   "id": "ce1d58ae-9866-4c0f-99b7-1147e589f975",
   "metadata": {
    "language": "python",
    "name": "ComparePValues"
   },
   "outputs": [],
   "source": "#how have the p-values changed\n\n#create a side-by-side comparison DataFrame\npvals_comparison = pd.DataFrame({\n    \"Full_Model_p\": model_full.pvalues,\n    \"Reduced_Model_p\": model_reduced.pvalues\n})\n\nprint(pvals_comparison)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19fd22fd-8b07-47c8-a473-f031b4566bcb",
   "metadata": {
    "name": "pValueCommentary",
    "collapsed": false
   },
   "source": "Variable\tFull Model p-value\tReduced Model p-value\tChange\nAge\t        0.167\t            0.065\t                ↓ More significant\nBlack\t    0.467\t\t        Removed                 (not significant)\nFemale\t    0.851\t\t        Removed                 (not significant)\nHS\t        0.940\t\t        Removed                 (not significant)\nIncome\t    0.070\t            0.007\t                ↓ More significant\nPrice\t    0.003\t            0.001\t                ↓ Slightly more significant\nconst\t    0.676\t            0.305\t                ↓ Still not significant\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "03c91ba1-52b8-474b-8f58-7bf9ea10d5c7",
   "metadata": {
    "name": "Section2Q2a",
    "collapsed": false
   },
   "source": "a)\tWhat does R2 tell you about the fit of the second model?\n    \n    •\tReduced model R² (0.303) means that 30.3% of the variation in cigarette sales across states is explained by the predictors in the reduced model (Age, Income, and Price).\n    \n\t•\tThis tells us that the model captures some structure in the data, yet nearly 70% of the variation remains unexplained — suggesting that other non-measured factors may likely influence cigarette sale, too.\n\n\nCompared to the Full Model:\n\t•\tThe full model had R² = 0.321, so removing variables caused only a small drop in explanatory power (about 1.8%).\n    \n\t•\tBut the adjusted R² actually increased from 0.228 → 0.259, which suggests the  REDUCED MODEL IS MORE EFFICIENT by explaining almost the same amount of variation with fewer, more meaningful predictors.\n\n\nThe reduced model, while not perfect, is statistically meaningful and simpler, which improves model quality."
  },
  {
   "cell_type": "markdown",
   "id": "2a7f9bfd-bd26-4517-9046-0bd12c44c48a",
   "metadata": {
    "name": "Section2Q2bAIC",
    "collapsed": false
   },
   "source": "Lower AIC values are better, because they indicate a model that fits the data well without unnecessary complexity (the balance of fit and complexity).  Lower AIC values indicate a better balance.  In our case the Full model AIC is 491.7 and the Reduced model AIC is 487.\n\nEven though the Reduced model uses fewer predictors, it performed better overall when accounting for complexity."
  },
  {
   "cell_type": "code",
   "id": "9582a00f-cb57-4f10-b248-7ddf12f7ab1d",
   "metadata": {
    "language": "python",
    "name": "Section2Q3Outliers",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#3)\tOutliers\n#a)\tDo a leverage plot. Are there influential outliers? \n\n#influence plot\nfig, ax = plt.subplots(figsize=(10, 6))\nsm.graphics.influence_plot(model_reduced, ax=ax, criterion=\"cooks\")\n\nplt.title(\"Influence Plot (Leverage vs. Studentized Residuals)\")\nplt.grid(True)\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f2c4d15-6cde-42be-8453-ed37bc734c7a",
   "metadata": {
    "name": "Section2Q3",
    "collapsed": false
   },
   "source": "Are there influential outliers?\n\nYes, I see a large bubble in the top left.  This is a high-leverage, high-residual point.  This an influential outlier and may disproportionately affect my model."
  },
  {
   "cell_type": "code",
   "id": "a99f3e46-fb24-4980-8831-38cffdc9fc82",
   "metadata": {
    "language": "python",
    "name": "Section2Q4Residuals"
   },
   "outputs": [],
   "source": "#4)\tLinearity & constant variance\n#   a)\tGenerate a predicted vs standardized residual plot.\n#       Does the data meet the linearity assumption?\n\n#import numpy as np\n#import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import OLSInfluence\n\n#compute influence\ninfluence = OLSInfluence(model_reduced)\n\n#extract fitted values and standardized residuals\nfitted_vals = model_reduced.fittedvalues\nstandardized_residuals = influence.resid_studentized_internal\n\n#Predicted vs Standardized Residuals plot\nplt.figure(figsize=(10, 6))\nplt.axhline(y=0, color='gray', linestyle='--', linewidth=1)\nplt.scatter(fitted_vals, standardized_residuals, alpha=0.7)\nplt.title(\"Predicted vs. Standardized Residuals\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Standardized Residuals\")\nplt.grid(True)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7aef971a-e1f5-4b08-9bcb-8d7e7bd3bab6",
   "metadata": {
    "name": "Section2Q4Linearity",
    "collapsed": false
   },
   "source": "Does the data meet the linearity assumption?\n\nThe data seems to reasonably meet the linearity assumption (by visual observation).\n\nThe residuals are scattered fairly randomly around the horizontal zero line. I see no \"strong\" curve or clear pattern.  If so, I'd lean toward non-linearity.  Furthermore, most of the standardized residuals fall within plus/minus 2, which I understand is common and expected.  I don't believe the few (moderate) outliers are enough to indicate a loss of linearity.\n"
  },
  {
   "cell_type": "code",
   "id": "cc38b569-fcda-4d7c-bbc3-1758972b87e8",
   "metadata": {
    "language": "python",
    "name": "Section2Q5a"
   },
   "outputs": [],
   "source": "#5)\tNormality\n#   a)\tDo a Q-Q Plot of the residuals.\n#   Are the residuals normally distributed?\n\n\n#get the residuals\nresiduals = model_reduced.resid\n\n# Generate the Q-Q plot\nsm.qqplot(residuals, line='45', fit=True)\nplt.title(\"Q-Q Plot of Residuals\")\nplt.xlabel(\"Theoretical Quantiles\")\nplt.ylabel(\"Sample Quantiles\")\nplt.grid(True)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c3fd895d-1f9a-4aa1-a68a-821f7b73bf3d",
   "metadata": {
    "name": "Section2Q5aDistribution",
    "collapsed": false
   },
   "source": "Are the residuals normally distributed?\n\nWell, mostly, they are normally distributed.  At the tail, we do see a couple of outliers (an indication of positive skew).  It's seemingly, normally distributed in the central portion.\n\nWith this data, the outliers could represent a problem/challenge to the normality of the residuals.  This is because in regression, a general rule of thumb is to have at least 10–15 observations per predictor to ensure stability in estimates. We may not have enough here.  I understand there are options for further cleansing and clarification, which I have not done here. "
  }
 ]
}