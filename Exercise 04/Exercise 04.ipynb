{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "vewo7xq3fwbrrdtx5vku",
   "authorId": "1043139642183",
   "authorName": "GASULLIVAN",
   "authorEmail": "sullivangregorya@wustl.edu",
   "sessionId": "5f3b19d0-923a-4037-b56b-afe56cabc73d",
   "lastEditTime": 1758493703172
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Section01",
    "collapsed": false
   },
   "source": "Section 1: Predicting used car prices\nWe’ll be using the cars.csv data set for this section of the exercise. The data set covers the characteristics and prices for used cars sold in India. We are interested in predicting the price of a car given some characteristics. We will attempt to build a linear regression model of Price. We are going to work on filling in the missing data that we previously dropped. \n\n1.\tTransform Price so that it looks more normal, produce histograms of the variable before and after transformation\n2.\tHow many values are missing for Power and Engine?\n3.\tWhich column has the most missing values and what should we do about it?\n4.\tBuild a model of transformed price based on Power, Mileage, Kilometers Driven, and Year, how much variance is explained?\n5.\tHow many rows were used to train the model?\n6.\tFill the missing values in Power and Mileage with their respective means and rebuild the model. Now how much variance is explained?\n7.\tHow many rows were used to train the model?\n8.\tImpute the missing data using MICE and rebuild the model\nMICE documentation: https://www.statsmodels.org/dev/generated/statsmodels.imputation.mice.MICE.html \n9.\tHow have the parameter estimates changed from step 4?\n10.\tPlot the distribution of Power with and without MICE\n11.\tPlot the distribution of Engine with and without MICE\n\n"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "ExamineTheData"
   },
   "source": "import pandas as pd\n\n#load cars.csv and customers.csv into pandas dataframes\ncars_df = pd.read_csv(\"cars.csv\")\ncustomers_df = pd.read_csv(\"customers.csv\")\n\n#print the first few rows of each dataframe (cars & customers)\nprint(cars_df.head())\nprint(customers_df.head())\n\n#generate some summary info (metadata, for example) about the dataframes\nprint(f\"Cars Shape: {cars_df.shape} (Rows, Columns)\")\nprint(f\"Customers Shape: {customers_df.shape} (Rows, Columns)\")\n\n#get the data types and non-null counts\nprint(\"Cars info:\")\nprint(cars_df.info())\nprint(\"Customers info:\")\nprint(customers_df.info())\n\n#get the summary statistics for both dataframes\nprint(\"Cars Summary Statistics for numeric columns\")\nprint(cars_df.describe())\n\nprint(\"Customers Summary Statistics for numeric columns\")\nprint(customers_df.describe())\n\n#check for missing data (by counting total missing values by column)\nprint(\"Missing Cars data:\")\nprint(cars_df.isnull().sum())\n\nprint(\"Missing Customers data:\")\nprint(customers_df.isnull().sum())\n\n#count unique values per column (might be helpful)\nprint(\"Unique Values per Cars column\")\nprint(cars_df.nunique())\n\nprint(\"Unique Values per Customers column\")\nprint(customers_df.nunique())\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6b9e4d6c-ee8d-41a2-8bcf-ceeee5128c6d",
   "metadata": {
    "language": "python",
    "name": "Q1PriceHistograms"
   },
   "outputs": [],
   "source": "#1.\tTransform Price so that it looks more normal, produce histograms of the variable before and after transformation\n\n#bring in some new libraries we need\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#clean and convert price from the Cars dataframe\ncars_df[\"Price\"] = pd.to_numeric(cars_df[\"Price\"], errors=\"coerce\")\ncars_df = cars_df.dropna(subset=[\"Price\"])  #drop rows with missing Price\n\n#plot original price histogram\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nsns.histplot(cars_df[\"Price\"], bins=30, kde=True)\nplt.title(\"Original Price Distribution\")\nplt.xlabel(\"Price\")\n\n#log-transform price to create a more normal-looking distribution\ncars_df[\"Log_Price\"] = np.log(cars_df[\"Price\"])\n\n#plot log-transformed price histogram\nplt.subplot(1, 2, 2)\nsns.histplot(cars_df[\"Log_Price\"], bins=30, kde=True, color=\"orange\")\nplt.title(\"Log-Transformed Price Distribution\")\nplt.xlabel(\"Log(Price)\")\n\n#display the price distribution histograms side-by-side\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ee1e2d90-2c1a-45fb-9a99-f8219319914d",
   "metadata": {
    "name": "Q2MissingValues",
    "collapsed": false
   },
   "source": "2. How many values are missing for Power and Engine?\n\nIn my ExamineTheData cell above, I have already tested for null values in all fields.  We see that there are 36 missing values in Power and 36 missing values in Engine.\n"
  },
  {
   "cell_type": "code",
   "id": "326ace06-8586-4b7c-b142-bed85ede6b16",
   "metadata": {
    "language": "python",
    "name": "Q3MostMissingValues"
   },
   "outputs": [],
   "source": "#3. Which column has the most missing values and what should we do about it?\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "790520ce-dd3f-45d6-a0e7-66ff23d918a5",
   "metadata": {
    "language": "python",
    "name": "Q3Code"
   },
   "outputs": [],
   "source": "num_rows = cars_df.shape[0]\n\nprint(f\"Missing values in cars dataframe out of {num_rows} rows:\")\nprint(cars_df.isnull().sum())\n\n#with this simple pandas df test I can display the null values for each column and visually see which has the highest count of missing values",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3c2e4c0-5940-4481-aa7d-a0eace3ca6b3",
   "metadata": {
    "language": "python",
    "name": "Q3CodeContinued"
   },
   "outputs": [],
   "source": "#while I can visually see the highest value above, I thought I'd attempt to determine it programmatically\n#furthermore, I'll make a function to do it for any given dataframe and also check it for the customers df\n\ndef report_max_missing(df, name=\"DataFrame\"):\n    \"\"\"\n    Prints the column with the most missing (NaN) values in the given DataFrame.\n    Parameters:\n        df (DataFrame): A pandas DataFrame\n        name (str): A name for the DataFrame (for labeling in the output)\n    \"\"\"\n    missing_counts = df.isnull().sum()\n    max_missing_col = missing_counts.idxmax()\n    max_missing_count = missing_counts.max()\n\n    print([name])\n    print(f\"\\nMax missing columns: {max_missing_count}\")\n    print(missing_counts)\n    #if missing_counts == 0 [wrong usage]:\n    #if missing_counts.empty: [wrong thing to test]\n    if max_missing_count == 0:\n        print(f\"\\n[name]: No missing values found\")\n    else:\n        print(f\"\\n{name} — Column with Most Missing Values:\")\n        print(f\"→ Column: {max_missing_col}\")\n    \n#perform the function on both datasets - cars and customers\nreport_max_missing(cars_df, \"Cars Dataset\")\nreport_max_missing(customers_df, \"Customers Dataset\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22c05327-b49c-443a-a897-e5f7058290c7",
   "metadata": {
    "language": "python",
    "name": "Q4ModelBuilding"
   },
   "outputs": [],
   "source": "#4.\tBuild a model of transformed price based on Power, Mileage, Kilometers Driven, and Year, how much variance is explained?\n\nimport statsmodels.api as sm\n\n#reload the Cars data since we're transforming from scratch\ncars_df = pd.read_csv(\"cars.csv\")\n\n#clean and convert necessary columns\ncars_df[\"Price\"] = pd.to_numeric(cars_df[\"Price\"], errors=\"coerce\")\ncars_df[\"Power\"] = cars_df[\"Power\"].str.replace(\" bhp\", \"\", regex=False)\ncars_df[\"Mileage\"] = cars_df[\"Mileage\"].str.replace(\" kmpl\", \"\", regex=False)\ncars_df[\"Mileage\"] = cars_df[\"Mileage\"].str.replace(\" km/g\", \"\", regex=False)\ncars_df[\"Engine\"] = cars_df[\"Engine\"].str.replace(\" CC\", \"\", regex=False)\n\ncars_df[\"Power\"] = pd.to_numeric(cars_df[\"Power\"], errors=\"coerce\")\ncars_df[\"Mileage\"] = pd.to_numeric(cars_df[\"Mileage\"], errors=\"coerce\")\ncars_df[\"Engine\"] = pd.to_numeric(cars_df[\"Engine\"], errors=\"coerce\")\n\n#drop rows where any of the key variables are missing\n#df_model = cars_df.dropna(subset=[\"Price\", \"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\"])\n    #I needed to change this to create a \"copy\" so I don't get a warning on the Price code that follows\ndf_model = cars_df.dropna(subset=[\"Price\", \"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\"]).copy()\n\n#transform Price by taking the log to normalize\ndf_model[\"Log_Price\"] = np.log(df_model[\"Price\"])\n\n#set up regression features and target\nX = df_model[[\"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\"]]\ny = df_model[\"Log_Price\"]\n\n#sdd constant (intercept)\nX = sm.add_constant(X)\n\n#fit linear regression model\nmodel = sm.OLS(y, X).fit()\nfirst_R2 = model.rsquared\n\n#R-squared\nprint(f\"R-squared (Variance Explained): {first_R2:.4f}\")\n\n#we'll need these later for the MICE comparison\nmodel4_params_df = pd.DataFrame({'Model 4 Coefficients': model.params.round(4)})\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a490b382-93ec-4544-a8a1-f3a74ce3d4ac",
   "metadata": {
    "name": "Q4ModelBuildingExplained",
    "collapsed": false
   },
   "source": "We have about 82% of the variability (in the log-transformed care prices) explained based on power, mileage, kilometers driven and year, which is a strong R-sqaured that indicates a good fit"
  },
  {
   "cell_type": "code",
   "id": "4ab98d7d-0471-45da-99bc-0d935ff3ea01",
   "metadata": {
    "language": "python",
    "name": "Q5TrainingRows"
   },
   "outputs": [],
   "source": "#5.\tHow many rows were used to train the model?\n\n#since we already cleaned the data, I need only to count the number of rows in the dataframe\nnum_rows = df_model.shape[0]\nprint(f\"Number of rows used to train the model: {num_rows}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ed70403-0e4d-4e1d-bfd9-48e86f205c55",
   "metadata": {
    "language": "python",
    "name": "Q6MeansModel"
   },
   "outputs": [],
   "source": "#6.\tFill the missing values in Power and Mileage with their respective means and rebuild the model. Now how much variance is explained?\n\n#reload the dataset since we're transforming anew\ncars_df = pd.read_csv(\"cars.csv\")\n\n#how many rows did we read in\nnum_rows = cars_df.shape[0]\nprint(f\"\\nNumber of rows read in from file: {num_rows}:\")\n\n\n#clean and strip units from strings, as before\ncars_df[\"Power\"] = cars_df[\"Power\"].str.replace(\" bhp\", \"\", regex=False)\ncars_df[\"Mileage\"] = cars_df[\"Mileage\"].str.replace(\" kmpl\", \"\", regex=False)\ncars_df[\"Mileage\"] = cars_df[\"Mileage\"].str.replace(\" km/g\", \"\", regex=False)\n\n#convert to numeric (invalid values will be NaN)\ncars_df[\"Power\"] = pd.to_numeric(cars_df[\"Power\"], errors=\"coerce\")\ncars_df[\"Mileage\"] = pd.to_numeric(cars_df[\"Mileage\"], errors=\"coerce\")\n\nnum_rows = cars_df.shape[0]\nprint(f\"\\nHow many rows after null conversions: {num_rows}:\")\n\n#calculate means, but skip missing values\nmean_power = cars_df[\"Power\"].mean(skipna=True)\nmean_mileage = cars_df[\"Mileage\"].mean(skipna=True)\n\n#now, go back through and replace missing values with the means we just calced\n#cars_df[\"Power\"].fillna(mean_power, inplace=True) #- got future warning on this code\ncars_df.fillna({\"Power\": mean_power}, inplace=True)\n\n#cars_df[\"Mileage\"].fillna(mean_mileage, inplace=True) #- got future warning on this code\ncars_df.fillna({\"Mileage\": mean_mileage}, inplace=True)\nnum_rows = cars_df.shape[0]\nprint(f\"\\nHow many rows after replacing nulls with means: {num_rows}:\")\n\n#show confirmation and results\nprint(f\"Mean Power used for imputation: {mean_power:.2f} bhp\")\nprint(f\"Mean Mileage used for imputation: {mean_mileage:.2f} kmpl\")\n\n# Optional: Check if any missing values remain\nprint(\"\\nMissing values after replacing missing values with means:\")\nprint(cars_df[[\"Power\", \"Mileage\"]].isna().sum())\n\ndf_model = cars_df.dropna(subset=[\"Price\", \"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\"]).copy()\n\n#log-transform the target variable (Price)\ndf_model[\"Log_Price\"] = np.log(df_model[\"Price\"])\n\n#define predictors and add constant for intercept\nX = df_model[[\"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\"]]\nX = sm.add_constant(X)\ny = df_model[\"Log_Price\"]\n\n#build the linear regression model\nmodel = sm.OLS(y, X).fit()\n\n#print R-squared (variance explained) and model summary\nprint(f\"R-squared (Variance Explained): {model.rsquared:.4f}\")\nprint(\"\\nModel Coefficients:\")\nprint(model.params)\n\n# Optional: preview predictions and residuals\n#df_model[\"Predicted_Log_Price\"] = model.predict(X)\n#df_model[\"Residuals\"] = df_model[\"Log_Price\"] - df_model[\"Predicted_Log_Price\"]\n\n#print(\"\\nFirst 5 Predictions:\")\n#print(df_model[\"Predicted_Log_Price\"].head())\n\n#print(\"\\nFirst 5 Residuals:\")\n#print(df_model[\"Residuals\"].head())\n\nnum_rows = df_model.shape[0]\nprint(f\"Number of rows used to train the new model: {num_rows}\")\nprint(f\"R-squared before replacing missing with averages: {first_R2}\")\nprint(f\"R-squared after replacing missing with averages: {model.rsquared}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c711967-f367-47f9-a839-9e8f8e3c2d9e",
   "metadata": {
    "name": "Q6Explained",
    "collapsed": false
   },
   "source": "We have very similar variance.  The value is slight lower when we replaced the missing values with the mean of the valid values."
  },
  {
   "cell_type": "code",
   "id": "e53825a5-0584-40b9-9b73-2a606fe14505",
   "metadata": {
    "language": "python",
    "name": "Q7"
   },
   "outputs": [],
   "source": "#7.\tHow many rows were used to train the model?\n\n#since we already cleaned the data, I need only to count the number of rows in the dataframe\nnum_rows = df_model.shape[0]\nprint(f\"Number of rows used to train the new model: {num_rows}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12d85b66-b526-4080-a6af-ae084b8a3f10",
   "metadata": {
    "name": "Q7Comments",
    "collapsed": false
   },
   "source": "We can see that all rows in the table were used since we replaced those rows with missing values, with the mean of the rows with valid values."
  },
  {
   "cell_type": "code",
   "id": "7ff7b0d9-dd6c-4cd4-b6fd-ae138bdda5d4",
   "metadata": {
    "language": "python",
    "name": "Q8"
   },
   "outputs": [],
   "source": "\n#Impute the missing data using MICE and rebuild the model\n\nfrom statsmodels.imputation.mice import MICEData\n\n#reload and clean data (AGAIN)\ncars_df = pd.read_csv(\"cars.csv\")\n\ncars_df[\"Power\"] = cars_df[\"Power\"].str.replace(\" bhp\", \"\", regex=False)\ncars_df[\"Mileage\"] = cars_df[\"Mileage\"].str.replace(\" kmpl\", \"\", regex=False)\ncars_df[\"Mileage\"] = cars_df[\"Mileage\"].str.replace(\" km/g\", \"\", regex=False)\ncars_df[\"Engine\"] = cars_df[\"Engine\"].str.replace(\" CC\", \"\", regex=False)\n\ncars_df[\"Power\"] = pd.to_numeric(cars_df[\"Power\"], errors=\"coerce\")\ncars_df[\"Mileage\"] = pd.to_numeric(cars_df[\"Mileage\"], errors=\"coerce\")\ncars_df[\"Engine\"] = pd.to_numeric(cars_df[\"Engine\"], errors=\"coerce\")\n\ncars_df = cars_df.dropna(subset=[\"Price\", \"Year\", \"Kilometers_Driven\"]).copy()\ncars_df[\"Log_Price\"] = np.log(cars_df[\"Price\"])\n\n#for relevant columns\nmodel_cols = [\"Log_Price\", \"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\"]\nmice_data = MICEData(cars_df[model_cols])\n\n#OLS on imputed data\nimp_df = mice_data.next_sample()\nX = sm.add_constant(imp_df[[\"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\"]])\ny = imp_df[\"Log_Price\"]\nmodel = sm.OLS(y, X).fit()\n\n#results\nprint(\"✅ Regression with MICE-imputed data:\\n\")\nprint(model.summary())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cef7cfa1-a25c-4359-8ee1-7a46cb7516cb",
   "metadata": {
    "language": "python",
    "name": "Q9"
   },
   "outputs": [],
   "source": "#9.\tHow have the parameter estimates changed from step 4?\nmice_params_df = pd.DataFrame({'MICE Coefficients': model.params.round(4)})\n\n\nprint(model4_params_df)\nprint(mice_params_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4efffd6-2ed9-46ee-b52d-5087a1cd9749",
   "metadata": {
    "name": "Q9Commentary",
    "collapsed": false
   },
   "source": "I don't see change between the two sets of coefficients.  It makes me wonder if I did something wrong.  This would seem to imply the imputed values via MICE are very similar to the means for the dataset.  Perhaps I need to take into account the missing values.  "
  },
  {
   "cell_type": "code",
   "id": "2ce04c25-ce92-4520-8680-3463af8a7327",
   "metadata": {
    "language": "python",
    "name": "Q10"
   },
   "outputs": [],
   "source": "#10. Plot the distribution of Power with and without MICE\n\n#reloading and recleaning to assure the proper data is being used for the plots\n\ndf_raw = pd.read_csv(\"cars.csv\")\n\n#clean\ndf_raw[\"Power\"] = df_raw[\"Power\"].str.replace(\" bhp\", \"\", regex=False)\ndf_raw[\"Power\"] = pd.to_numeric(df_raw[\"Power\"], errors=\"coerce\")\n\n#before imputation\npower_before = df_raw[\"Power\"].copy()\n\n#prepare MICE data\ndf_for_mice = df_raw[[\"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\", \"Price\"]].copy()\ndf_for_mice[\"Mileage\"] = df_for_mice[\"Mileage\"].str.replace(\" kmpl\", \"\", regex=False)\ndf_for_mice[\"Mileage\"] = df_for_mice[\"Mileage\"].str.replace(\" km/g\", \"\", regex=False)\ndf_for_mice[\"Mileage\"] = pd.to_numeric(df_for_mice[\"Mileage\"], errors=\"coerce\")\ndf_for_mice = df_for_mice.dropna(subset=[\"Price\", \"Year\", \"Kilometers_Driven\"]).copy()\ndf_for_mice[\"Log_Price\"] = np.log(df_for_mice[\"Price\"])\n\n#apply MICE imputation\nmice_data = MICEData(df_for_mice[[\"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\", \"Log_Price\"]])\ndf_mice = mice_data.next_sample()\n\n#after imputation\npower_after = df_mice[\"Power\"]\n\n#plot both distributions, with side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\n#before MICE\nsns.histplot(power_before, ax=axes[0], color=\"skyblue\", kde=True, bins=30)\naxes[0].set_title(\"Power Distribution: Before MICE\")\naxes[0].set_xlabel(\"Power (bhp)\")\naxes[0].set_ylabel(\"Density\")\naxes[0].grid(True)\n\n#after MICE\nsns.histplot(power_after, ax=axes[1], color=\"orange\", kde=True, bins=30)\naxes[1].set_title(\"Power Distribution: After MICE\")\naxes[1].set_xlabel(\"Power (bhp)\")\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c03464e4-5db6-4664-b7f8-6647fa4d6482",
   "metadata": {
    "name": "Q10Commentary",
    "collapsed": false
   },
   "source": "As we observed with the coefficent commentary above, we do not see a visually-different view with the histograms side-by-side"
  },
  {
   "cell_type": "code",
   "id": "a9c1d8f2-4e6e-4cd1-bba4-b160c0bbec79",
   "metadata": {
    "language": "python",
    "name": "Q11"
   },
   "outputs": [],
   "source": "#11. Plot the distribution of Engine with and without MICE\n\n\n#reload and clean the dataset\ndf_raw = pd.read_csv(\"cars.csv\")\n\n#clean Engine column\ndf_raw[\"Engine\"] = df_raw[\"Engine\"].str.replace(\" CC\", \"\", regex=False)\ndf_raw[\"Engine\"] = pd.to_numeric(df_raw[\"Engine\"], errors=\"coerce\")\n\n#save Engine before imputation\nengine_before = df_raw[\"Engine\"].copy()\n\n#clean other required columns\ndf_raw[\"Power\"] = df_raw[\"Power\"].str.replace(\" bhp\", \"\", regex=False)\ndf_raw[\"Power\"] = pd.to_numeric(df_raw[\"Power\"], errors=\"coerce\")\ndf_raw[\"Mileage\"] = df_raw[\"Mileage\"].str.replace(\" kmpl\", \"\", regex=False)\ndf_raw[\"Mileage\"] = df_raw[\"Mileage\"].str.replace(\" km/g\", \"\", regex=False)\ndf_raw[\"Mileage\"] = pd.to_numeric(df_raw[\"Mileage\"], errors=\"coerce\")\n\n#drop rows required for model training\ndf_for_mice = df_raw.dropna(subset=[\"Price\", \"Year\", \"Kilometers_Driven\"]).copy()\ndf_for_mice[\"Log_Price\"] = np.log(df_for_mice[\"Price\"])\n\n#prepare MICE input\nmice_data = MICEData(df_for_mice[[\"Engine\", \"Power\", \"Mileage\", \"Kilometers_Driven\", \"Year\", \"Log_Price\"]])\ndf_mice = mice_data.next_sample()\n\n#save Engine after MICE\nengine_after = df_mice[\"Engine\"]\n\n#plot side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\n#before MICE\nsns.histplot(engine_before, ax=axes[0], color=\"lightblue\", kde=True, bins=30)\naxes[0].set_title(\"Engine Distribution: Before MICE\")\naxes[0].set_xlabel(\"Engine (CC)\")\naxes[0].set_ylabel(\"Density\")\naxes[0].grid(True)\n\n#after MICE\nsns.histplot(engine_after, ax=axes[1], color=\"coral\", kde=True, bins=30)\naxes[1].set_title(\"Engine Distribution: After MICE\")\naxes[1].set_xlabel(\"Engine (CC)\")\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd16641b-74fa-4820-8dd3-02dfab75c6b6",
   "metadata": {
    "name": "Section2",
    "collapsed": false
   },
   "source": "Section 2: Predicting customer spending\nWe’ll be using the customers.csv data set for this exercise. The data set covers the demographic characteristics of some customers and the amount they spent over the past year at an online retailer. For this exercise it is recommended to use the sklearn packages for linear regression, ridge, and lasso. Sklearn documentation linked below.\nLinear regression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\nRidge: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html \nLasso: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\nIn order to interact the categorical variables you will need to dummy code them and manually multiply, an example is given below. \ncustomerDf = pd.get_dummies(data=customerDf, columns=['sex', 'race'], prefix=['sex','race'])\ncustomerDf[\"Hispanic_Male\"] = np.multiply(customerDf[\"race_hispanic\"],customerDf[\"sex_male\"])\n\n\n1.\tBuild a linear regression with all the dependent variables and two way interactions between sex and race, consider the other category for race and sex to be the reference category and treat it appropriately\n2.\tBuild ridge models with various values for alpha. Create a chart showing how the coefficients change with alpha values\n3.\tBuild lasso models with various values for alpha. Create a chart showing how the coefficients change with alpha values\n4.\tCompare the coefficients from linear regression, ridge, and lasso (select an alpha value using your chart)\n5.\tCompare the R2 from lr, ridge, and lasso\n6.\tWhich model would you choose, and why?\n7.\tWhich variables are dropped from the chosen model that were not dropped in linear regression?\n"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Section2Code"
   },
   "source": "#1. Build a linear regression with all the depndent variables and two way interactions between sex and race, \n#   consider the other category for race and sex to be the reference category and treat it appropriately\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n#load the dataset\ndf = pd.read_csv(\"customers.csv\")\n\n#dummy encode 'sex' and 'race' with 'other' as reference\ndf = pd.get_dummies(df, columns=['sex', 'race'], prefix=['sex', 'race'], drop_first=True)\n\n#create interaction terms between sex and race (e.g., male * each race)\n#for example: sex_male * race_black, sex_male * race_white, sex_male * race_hispanic (if present)\ninteraction_terms = []\nif 'sex_male' in df.columns:\n    for col in df.columns:\n        if col.startswith('race_'):\n            interaction_col = f\"{col}_x_sex_male\"\n            df[interaction_col] = df[col] * df['sex_male']\n            interaction_terms.append(interaction_col)\n\n#prepare X and y, excluding the target and any columns not needed in model\ntarget = \"spend\"\nX_cols = [col for col in df.columns if col != target]\nX = df[X_cols]\ny = df[target]\n\n#fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n#print coefficients and R^2\ncoefficients = pd.Series(model.coef_, index=X.columns)\nintercept = model.intercept_\nr2 = model.score(X, y)\n\nprint(\"Intercept:\", intercept)\nprint(\"\\nCoefficients:\\n\", coefficients)\nprint(\"\\nR-squared:\", round(r2, 4))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4b94ea93-2def-49e0-b4ff-8d40748dcb5b",
   "metadata": {
    "language": "python",
    "name": "Sec2Q2"
   },
   "outputs": [],
   "source": "#2.\tBuild ridge models with various values for alpha. Create a chart showing how the coefficients change with alpha values\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n#reload the dataset\ndf = pd.read_csv(\"customers.csv\")\n\n#dummy encode sex and race (drop_first=True for reference category)\ndf = pd.get_dummies(df, columns=['sex', 'race'], prefix=['sex','race'], drop_first=True)\n\n#create interaction terms: sex_male * race_*\ninteraction_terms = []\nif 'sex_male' in df.columns:\n    for col in df.columns:\n        if col.startswith('race_'):\n            interaction_col = f\"{col}_x_sex_male\"\n            df[interaction_col] = df[col] * df['sex_male']\n            interaction_terms.append(interaction_col)\n\n#define predictors (X) and target (y)\ntarget = \"spend\"\nX_cols = [col for col in df.columns if col != target]\nX = df[X_cols]\ny = df[target]\n\n#standardize X (optional, but common for Ridge/Lasso)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Try a range of alphas and record coefficients\nalphas = np.logspace(-2, 4, 50)\ncoefs = []\n\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_scaled, y)\n    coefs.append(ridge.coef_)\n\n#plotting coefficient paths\nplt.figure(figsize=(12, 6))\ncoefs = np.array(coefs)\n\nfor i in range(coefs.shape[1]):\n    plt.plot(alphas, coefs[:, i], label=X.columns[i])\n\nplt.xscale(\"log\")\nplt.xlabel(\"Alpha (log scale)\")\nplt.ylabel(\"Coefficient value\")\nplt.title(\"Ridge Coefficients vs. Alpha\")\nplt.axhline(0, color='black', linestyle='--', linewidth=1)\nplt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1), fontsize=\"small\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c201a6bf-8e50-4abc-9e05-8507386fa8d0",
   "metadata": {
    "language": "python",
    "name": "BestRidgeAlpha"
   },
   "outputs": [],
   "source": "from sklearn.linear_model import RidgeCV\n\nridge_alphas = [0.01, 0.1, 1, 10, 100]\nridge_cv = RidgeCV(alphas=ridge_alphas, store_cv_results=True)\nridge_cv.fit(X_scaled, y)\nprint(\"Best Ridge alpha:\", ridge_cv.alpha_)\n\n#I didn't end up using this value as it's off by a factor of 100 (I'm still trying to understand why)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ec48ec5-199e-4d01-b269-d8785ad21526",
   "metadata": {
    "language": "python",
    "name": "BestLassoAlpha"
   },
   "outputs": [],
   "source": "from sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV(alphas=[0.001, 0.01, 0.1, 1, 10], cv=5, max_iter=10000)\nlasso_cv.fit(X_scaled, y)\nprint(\"Best Lasso alpha:\", lasso_cv.alpha_)\n\n#I didn't end up using this value as it's off by a factor of 100 (I'm still trying to understand why)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acb1272e-1b58-40e5-90dd-c240825b1418",
   "metadata": {
    "language": "python",
    "name": "Sec2Q3"
   },
   "outputs": [],
   "source": "#3.\tBuild lasso models with various values for alpha. Create a chart showing how the coefficients change with alpha values\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n#relLoad the dataset\ndf = pd.read_csv(\"customers.csv\")\n\n#encode 'sex' and 'race' (drop_first=True to set reference categories)\ndf = pd.get_dummies(df, columns=['sex', 'race'], prefix=['sex', 'race'], drop_first=True)\n\n#create interaction terms: sex_male * race_*\ninteraction_terms = []\nif 'sex_male' in df.columns:\n    for col in df.columns:\n        if col.startswith('race_'):\n            interaction_col = f\"{col}_x_sex_male\"\n            df[interaction_col] = df[col] * df['sex_male']\n            interaction_terms.append(interaction_col)\n\n#set up X and y\ntarget = \"spend\"\nX_cols = [col for col in df.columns if col != target]\nX = df[X_cols]\ny = df[target]\n\n#standardize X\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n#Lasso: Loop over alphas and store coefficients\nalphas = np.logspace(-2, 1, 50)  # from 0.01 to 10\ncoefs = []\n\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha, max_iter=10000)\n    lasso.fit(X_scaled, y)\n    coefs.append(lasso.coef_)\n\n#plot coefficient paths\nplt.figure(figsize=(12, 6))\ncoefs = np.array(coefs)\n\nfor i in range(coefs.shape[1]):\n    plt.plot(alphas, coefs[:, i], label=X.columns[i])\n\nplt.xscale(\"log\")\nplt.xlabel(\"Alpha (log scale)\")\nplt.ylabel(\"Coefficient Value\")\nplt.title(\"Lasso Coefficients vs. Alpha\")\nplt.axhline(0, color='black', linestyle='--', linewidth=1)\nplt.legend(loc=\"best\", bbox_to_anchor=(1.05, 1), fontsize=\"small\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9009e6d-cb9b-4a1b-8b37-3d01a31086a3",
   "metadata": {
    "language": "python",
    "name": "Sec2Q4"
   },
   "outputs": [],
   "source": "#4.\tCompare the coefficients from linear regression, ridge, and lasso (select an alpha value using your chart)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler\n\n#reload data\ndf = pd.read_csv(\"customers.csv\")\n\n#dummy encode 'sex' and 'race' and create interaction terms\ndf = pd.get_dummies(df, columns=['sex', 'race'], prefix=['sex','race'], drop_first=True)\n\nif 'sex_male' in df.columns:\n    for col in df.columns:\n        if col.startswith('race_'):\n            df[f'{col}_x_sex_male'] = df[col] * df['sex_male']\n\n#set up X and y\ntarget = \"spend\"\nX_cols = [col for col in df.columns if col != target]\nX = df[X_cols]\ny = df[target]\n\n#standardize predictors\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n#fit all models to be compared\nlinear_model = LinearRegression().fit(X_scaled, y)\nridge_model = Ridge(alpha=10).fit(X_scaled, y)   # <- Picked alpha=10 from ridge plot\nlasso_model = Lasso(alpha=0.1, max_iter=10000).fit(X_scaled, y)  # <- Picked alpha=0.1 from lasso plot\n\n#create comparison DataFrame\ncoef_comparison = pd.DataFrame({\n    'Feature': X.columns,\n    'Linear': linear_model.coef_,\n    'Ridge (α=10)': ridge_model.coef_,\n    'Lasso (α=0.1)': lasso_model.coef_,\n})\n\n#round and sort by absolute size of Linear Regression coef\ncoef_comparison = coef_comparison.round(4).set_index(\"Feature\")\ncoef_comparison = coef_comparison.reindex(coef_comparison['Linear'].abs().sort_values(ascending=False).index)\n\nprint(coef_comparison)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59dbf2b3-cd33-4d65-964e-9f68d3c8206a",
   "metadata": {
    "language": "python",
    "name": "Sec2Q4Again"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler\n\n#prepare the data\ndf = pd.read_csv(\"customers.csv\")\n\n#encode categorical variables\ndf = pd.get_dummies(df, columns=['sex', 'race'], prefix=['sex','race'], drop_first=True)\n\n#create interaction term\nif 'sex_male' in df.columns:\n    for col in df.columns:\n        if col.startswith('race_'):\n            df[f'{col}_x_sex_male'] = df[col] * df['sex_male']\n\n#setup X and y\ntarget = 'spend'\nX = df.drop(columns=[target])\ny = df[target]\n\n#standardize X\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n#fit models\nmodels = {\n    'Linear Regression': LinearRegression(),\n    'Ridge (α=10)': Ridge(alpha=10),\n    'Lasso (α=0.1)': Lasso(alpha=0.1, max_iter=10000)\n}\n\ncoef_df = pd.DataFrame(index=X.columns)\n\nfor name, model in models.items():\n    model.fit(X_scaled, y)\n    coef_df[name] = model.coef_\n\n#plot the coefficients\ncoef_df.plot(kind='bar', figsize=(12, 6))\nplt.title('Model Coefficient Comparison')\nplt.xlabel('Features')\nplt.ylabel('Coefficient Value')\nplt.xticks(rotation=45, ha='right')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.grid(True)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30f659a1-13a7-4218-9ca5-36c23966170d",
   "metadata": {
    "language": "python",
    "name": "Sec2Q5"
   },
   "outputs": [],
   "source": "#5.\tCompare the R2 from lr, ridge, and lasso\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import r2_score\n\n#Linear Regression\nlinear_model = LinearRegression().fit(X_scaled, y)\ny_pred_linear = linear_model.predict(X_scaled)\nr2_linear = r2_score(y, y_pred_linear)\n\n#Ridge Regression (choose alpha based on your ridge plot)\nridge_model = Ridge(alpha=10).fit(X_scaled, y)\ny_pred_ridge = ridge_model.predict(X_scaled)\nr2_ridge = r2_score(y, y_pred_ridge)\n\n#Lasso Regression (choose alpha based on your lasso plot)\nlasso_model = Lasso(alpha=0.1, max_iter=10000).fit(X_scaled, y)\ny_pred_lasso = lasso_model.predict(X_scaled)\nr2_lasso = r2_score(y, y_pred_lasso)\n\n#print R² comparison\nprint(\"R² Comparison:\")\nprint(f\"Linear Regression R²: {r2_linear:.4f}\")\nprint(f\"Ridge Regression R²:  {r2_ridge:.4f}  (alpha=10)\")\nprint(f\"Lasso Regression R²:  {r2_lasso:.4f}  (alpha=0.1)\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4edb8db-ef4a-40a6-82aa-187cb913dd74",
   "metadata": {
    "name": "Sec2Q5Commentaryt",
    "collapsed": false
   },
   "source": "The r-squared across all models are nearly identical.  All three models explain about 87% of the variance is customer spending.  The LR model \n"
  },
  {
   "cell_type": "markdown",
   "id": "9fcfeab4-ff9a-4549-8d79-fd3e52d0873a",
   "metadata": {
    "name": "Sec2Q6",
    "collapsed": false
   },
   "source": "#6.\tWhich model would you choose, and why?\n\nGiven the high r2 arcross all models and the small difference in performance, there is no clear sign of overfitting in the set.  Therefore, I choose the Ridge Regression as it gives me a nearly identical  accuracy as the LR and it retains all featuree (unlike Lasso), which may end up being desirable.\n"
  },
  {
   "cell_type": "markdown",
   "id": "56ba4df0-53a5-4578-a5dc-b7853e993277",
   "metadata": {
    "name": "Sec2Q7",
    "collapsed": false
   },
   "source": "#7.\tWhich variables are dropped from the chosen model that were not dropped in linear regression?\n\nI don't see that any variabls were dropped from Ridge that were not dropped in LR.  The question would seem to imply there should have been. We know Ridge shrinks some coefficients toward zero, but I don't see that any are dropped entirely.  Here are the features that Ridge pulled slightly closer to zero to help prevent overfitting:\n\nVariable                 Amount Shrunk\nrace_white_x_sex_male    0.0761\nage                      0.0509\nrace_black_x_sex_male    0.0463\nrace_hispanic_x_sex_male 0.0459\nrace_other_x_sex_male    0.0402\nsex_other                0.0050\nincome                   0.0021\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "07be037f-3246-4557-b4d8-eea1e8611cfb",
   "metadata": {
    "language": "python",
    "name": "CofficientShrinkageTable"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split\n\n# Define target and predictors\ntarget_column = \"spend\"  # Correct column name\nX = df.drop(columns=[target_column])\ny = df[target_column]\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Linear regression\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\nlinear_coef = pd.Series(lr_model.coef_, index=X.columns)\n\n# Ridge regression\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_train, y_train)\nridge_coef = pd.Series(ridge_model.coef_, index=X.columns)\n\n# Compare shrinkage\nshrinkage = linear_coef - ridge_coef\nshrunk_coefficients = shrinkage[shrinkage > 0]\n\n# Create comparison table\nshrinkage_table = pd.DataFrame({\n    \"Variable\": shrunk_coefficients.index,\n    \"Linear Regression\": linear_coef[shrunk_coefficients.index].values,\n    \"Ridge\": ridge_coef[shrunk_coefficients.index].values,\n    \"Amount Shrunk\": shrunk_coefficients.values\n}).sort_values(by=\"Amount Shrunk\", ascending=False).reset_index(drop=True)\n\nshrinkage_table",
   "execution_count": null
  }
 ]
}