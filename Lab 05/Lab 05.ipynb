{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "csezv6tplhh3w6vtu27v",
   "authorId": "1043139642183",
   "authorName": "GASULLIVAN",
   "authorEmail": "sullivangregorya@wustl.edu",
   "sessionId": "b308a84b-7d84-4d17-99d3-6e8c8f0410ac",
   "lastEditTime": 1758594972113
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e00c27-b97d-41c2-85f6-92ac5885ea98",
   "metadata": {
    "name": "Lab05",
    "collapsed": false
   },
   "source": "Section 1: Multiple Regression Walk Through\nCybersecurity incidents are an increasing threat to safe and responsible business operations, but even the critical job of protecting sensitive data doesn’t come with an unlimited budget. Where should you invest funds on cybersecurity enforcement across different departments?\n•\tWhy are some departments getting hit harder than others?\n•\tWhich risk factors matter the most?\n•\tCan we predict which departments are likely to see more incidents in the near future?\n\nBreak your work up in your lab notebook the following stages using the outline below.\n\n1.\tData Familiarization & Hygiene\n    a.\tInspect schema, ranges, and datatypes.\n    b.\tDetect and fix issues: missing values, impossible percentages, and text in numeric fields.\n    c.\tApply simple imputation and clipping strategies.\n2.\tExploratory Analysis (EDA)\n    a.\tPlot distributions (incidents histogram).\n    b.\tExplore relationships (incidents vs. vulnerabilities, patch time).\n    c.\tSpot skewness and potential transformations.\n3.\tFeature Engineering & Confounders\n    a.\tCreate transformations (log_org_size, log_vulns).\n    b.\tEngineer interaction terms (e.g., vulnerabilities × endpoint coverage gaps).\n    c.\tDiscuss how org size and budget confound relationships.\n4.\tModeling – Naïve vs. Improved\n    a.\tFit a baseline OLS model without confounders.\n    b.\tFit an improved OLS model with transformations, confounders, and interactions.\n    c.\tCompare coefficients, R2, AIC/BIC.\n5.\tAssumptions Diagnostics\n    a.\tCheck linearity, normality, and homoscedasticity (residual plots, QQ plot, Breusch–Pagan).\n    b.\tEvaluate multicollinearity with VIF.\n6.\tOutliers & Influence\n    a.\tIdentify leverage points and high Cook’s D values.\n    b.\tDiscuss whether to drop or retain influential cases.\n    c.\tRefit model and compare metrics.\n7.\tGeneralization Check\n    a.\tTrain/test split; calculate RMSE on test data.\n    b.\tReflect on in-sample vs. out-of-sample fit.\n8.\tThink About:\n    a.\tWhich variables appear most predictive of incidents?\n    b.\tHow do confounders shift interpretations?\n    c.\tWhat model would you present to leadership—and what caveats remain?\n"
  },
  {
   "cell_type": "markdown",
   "id": "89cde704-a723-4a4e-ac99-e573a1de2523",
   "metadata": {
    "name": "DataFamiliarization",
    "collapsed": false
   },
   "source": "1.\tData Familiarization & Hygiene\n    a.\tInspect schema, ranges, and datatypes.\n    b.\tDetect and fix issues: missing values, impossible percentages, and text in numeric fields.\n    c.\tApply simple imputation and clipping strategies.\n\n"
  },
  {
   "cell_type": "code",
   "id": "d4eb829d-69b2-4e62-9400-360f38b667f9",
   "metadata": {
    "language": "python",
    "name": "DataFamiliarizationCode"
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# Load CSV\ndf = pd.read_csv(\"cybersec.csv\")\n\n# 1a. Inspect schema and types\nprint(\"Schema and Data Types:\\n\", df.dtypes, \"\\n\")\n\n# Summary statistics\nprint(\"Summary Statistics:\\n\", df.describe(include='all'), \"\\n\")\n\n# Missing values\nprint(\"Missing Values:\\n\", df.isnull().sum(), \"\\n\")\n\n# Sample unique values for object columns\nobject_columns = df.select_dtypes(include=['object']).columns\nfor col in object_columns:\n    print(f\"{col} unique values:\", df[col].unique()[:10])\nprint()\n\n# 1b. Detect & fix issues\n# Clip impossible % values\ndf['phishing_sim_click_rate'] = df['phishing_sim_click_rate'].clip(upper=1.0)\n\n# Fix text 'null' if present (not needed here, but included as good hygiene)\ndf.replace(\"null\", pd.NA, inplace=True)\n\n# 1c. Impute missing values (mean)\ndf['vuln_count'] = df['vuln_count'].fillna(df['vuln_count'].mean())\ndf['training_completion_rate'] = df['training_completion_rate'].fillna(df['training_completion_rate'].mean())\n\n# Check if all missing values handled\nprint(\"Post-cleaning Missing Values:\\n\", df.isnull().sum())\n\n# Optional preview\ndf.head()\n\n# actual column names\nprint(df.columns.tolist())\n\n# Check the min and max values of the endpoint_coverage column\nmin_coverage = df['endpoint_coverage'].min()\nmax_coverage = df['endpoint_coverage'].max()\n\nprint(f\"Endpoint Coverage ranges from {min_coverage}% to {max_coverage}%.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2fbb9f3b-4b7e-42ef-8d54-a69247b61682",
   "metadata": {
    "name": "EDA",
    "collapsed": false
   },
   "source": "2.\tExploratory Analysis (EDA)\n    a.\tPlot distributions (incidents histogram).\n    b.\tExplore relationships (incidents vs. vulnerabilities, patch time).\n    c.\tSpot skewness and potential transformations.\n"
  },
  {
   "cell_type": "code",
   "id": "31c8df66-7ef6-48e1-b3c6-bf722ba30f94",
   "metadata": {
    "language": "python",
    "name": "EDACode"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import skew\n\n# Load the data\ndf = pd.read_csv('cybersec.csv')  # Adjust if loading differently in Snowflake UI\n\n# --- 2a: Histogram of Security Incidents ---\nplt.figure(figsize=(8, 5))\nsns.histplot(df['security_incidents'], bins=30, kde=True)\nplt.title(\"Histogram of Security Incidents (Original)\")\nplt.xlabel(\"Number of Security Incidents\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# --- 2b: Explore relationships via scatter plots ---\n\n# Incidents vs. vuln_count\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x='vuln_count', y='security_incidents', data=df)\nplt.title(\"Security Incidents vs. Vulnerability Count\")\nplt.xlabel(\"Vulnerability Count\")\nplt.ylabel(\"Security Incidents\")\nplt.show()\n\n# Incidents vs. mean_time_to_patch\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x='mean_time_to_patch', y='security_incidents', data=df)\nplt.title(\"Security Incidents vs. Mean Time to Patch\")\nplt.xlabel(\"Mean Time to Patch (days)\")\nplt.ylabel(\"Security Incidents\")\nplt.show()\n\n# --- 2c: Skewness and Log Transformation ---\n# Original skewness\norig_skew = skew(df['security_incidents'].dropna())\nprint(f\"Skewness of security_incidents (original): {orig_skew:.2f}\")\n\n# Log transform\ndf['log_security_incidents'] = np.log1p(df['security_incidents'])\n\n# Skewness after transformation\nlog_skew = skew(df['log_security_incidents'].dropna())\nprint(f\"Skewness after log transformation: {log_skew:.2f}\")\n\n# Plot log-transformed histogram\nplt.figure(figsize=(8, 5))\nsns.histplot(df['log_security_incidents'], bins=30, kde=True)\nplt.title(\"Histogram of Log-Transformed Security Incidents\")\nplt.xlabel(\"log(1 + Security Incidents)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5402025d-d75a-4508-886e-5eaf2dfaa249",
   "metadata": {
    "name": "Confounders",
    "collapsed": false
   },
   "source": "3.\tFeature Engineering & Confounders\n    a.\tCreate transformations (log_org_size, log_vulns).\n    b.\tEngineer interaction terms (e.g., vulnerabilities × endpoint coverage gaps).\n    c.\tDiscuss how org size and budget confound relationships.\n"
  },
  {
   "cell_type": "code",
   "id": "1ae368a9-a17d-4041-aa31-acecf560c0dc",
   "metadata": {
    "language": "python",
    "name": "ConfoundersCode3a"
   },
   "outputs": [],
   "source": "# Create log-transformed columns\ndf['log_org_size'] = np.log1p(df['org_size'])\ndf['log_vulns'] = np.log1p(df['vuln_count'])\n\n# Visualize histograms\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(df['log_org_size'], bins=30, kde=True, ax=axs[0])\naxs[0].set_title(\"Log-transformed Org Size\")\n\nsns.histplot(df['log_vulns'], bins=30, kde=True, ax=axs[1])\naxs[1].set_title(\"Log-transformed Vulnerability Count\")\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8592868-1c75-40a8-8843-675693fcf9e2",
   "metadata": {
    "language": "python",
    "name": "ConfoundersCode3b"
   },
   "outputs": [],
   "source": "# Create interaction term\ndf['vuln_x_endpoint'] = df['vuln_count'] * (1 - df['endpoint_coverage'])\nprint(df['endpoint_coverage'])\n\n# Optionally visualize\nsns.scatterplot(x='vuln_x_endpoint', y='security_incidents', data=df)\nplt.title(\"Security Incidents vs. Vulnerability × Endpoint Gap Interaction\")\nplt.xlabel(\"Vulnerability × Endpoint Gap\")\nplt.ylabel(\"Security Incidents\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2aa5f112-7c4f-4acf-b8bb-5aa9ea323c35",
   "metadata": {
    "name": "Modeling",
    "collapsed": false
   },
   "source": "4.\tModeling – Naïve vs. Improved\n    a.\tFit a baseline OLS model without confounders.\n    b.\tFit an improved OLS model with transformations, confounders, and interactions.\n    c.\tCompare coefficients, R2, AIC/BIC.\n"
  },
  {
   "cell_type": "code",
   "id": "09976fa7-7aad-4d79-a8ee-34f579d65b11",
   "metadata": {
    "language": "python",
    "name": "ModelingCode"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cad5210d-e40c-447c-bcd7-5c0794aef91c",
   "metadata": {
    "name": "Assumptions",
    "collapsed": false
   },
   "source": "5.\tAssumptions Diagnostics\n    a.\tCheck linearity, normality, and homoscedasticity (residual plots, QQ plot, Breusch–Pagan).\n    b.\tEvaluate multicollinearity with VIF.\n"
  },
  {
   "cell_type": "code",
   "id": "e8672ae4-a9be-4433-b8b6-3538b0689418",
   "metadata": {
    "language": "python",
    "name": "AssumptionsCode"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e47bb093-3a35-4c62-9544-83d9853e0159",
   "metadata": {
    "name": "Outliers",
    "collapsed": false
   },
   "source": "6.\tOutliers & Influence\n    a.\tIdentify leverage points and high Cook’s D values.\n    b.\tDiscuss whether to drop or retain influential cases.\n    c.\tRefit model and compare metrics.\n"
  },
  {
   "cell_type": "code",
   "id": "ea1bf099-da69-41c3-bf21-5633599a7839",
   "metadata": {
    "language": "python",
    "name": "OutliersCode"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3602c0fc-2848-4ae1-8062-db53acef5afb",
   "metadata": {
    "name": "Generalization",
    "collapsed": false
   },
   "source": "7.\tGeneralization Check\n    a.\tTrain/test split; calculate RMSE on test data.\n    b.\tReflect on in-sample vs. out-of-sample fit.\n"
  },
  {
   "cell_type": "code",
   "id": "cc18fd08-522e-4fe3-82c0-1b74c178d261",
   "metadata": {
    "language": "python",
    "name": "GeneralizationCode"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c67fa342-22a4-4d20-9a35-296762a42caf",
   "metadata": {
    "name": "ThinkAbout",
    "collapsed": false
   },
   "source": "8.\tThink About:\n    a.\tWhich variables appear most predictive of incidents?\n    b.\tHow do confounders shift interpretations?\n    c.\tWhat model would you present to leadership—and what caveats remain?"
  },
  {
   "cell_type": "code",
   "id": "5b910b4f-6f61-4ca4-8b55-cea7b748cf9d",
   "metadata": {
    "language": "python",
    "name": "ThinkAboutCode"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}