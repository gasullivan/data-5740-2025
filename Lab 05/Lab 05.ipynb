{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "a7nrwheldjwo6st4o6jn",
   "authorId": "1043139642183",
   "authorName": "GASULLIVAN",
   "authorEmail": "sullivangregorya@wustl.edu",
   "sessionId": "1d3f2d69-de62-47be-9479-9123a02218a4",
   "lastEditTime": 1759262972739
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e00c27-b97d-41c2-85f6-92ac5885ea98",
   "metadata": {
    "name": "Lab05",
    "collapsed": false
   },
   "source": "Section 1: Multiple Regression Walk Through\nCybersecurity incidents are an increasing threat to safe and responsible business operations, but even the critical job of protecting sensitive data doesn’t come with an unlimited budget. Where should you invest funds on cybersecurity enforcement across different departments?\n•\tWhy are some departments getting hit harder than others?\n•\tWhich risk factors matter the most?\n•\tCan we predict which departments are likely to see more incidents in the near future?\n\nBreak your work up in your lab notebook the following stages using the outline below.\n\n1.\tData Familiarization & Hygiene\n    a.\tInspect schema, ranges, and datatypes.\n    b.\tDetect and fix issues: missing values, impossible percentages, and text in numeric fields.\n    c.\tApply simple imputation and clipping strategies.\n2.\tExploratory Analysis (EDA)\n    a.\tPlot distributions (incidents histogram).\n    b.\tExplore relationships (incidents vs. vulnerabilities, patch time).\n    c.\tSpot skewness and potential transformations.\n3.\tFeature Engineering & Confounders\n    a.\tCreate transformations (log_org_size, log_vulns).\n    b.\tEngineer interaction terms (e.g., vulnerabilities × endpoint coverage gaps).\n    c.\tDiscuss how org size and budget confound relationships.\n4.\tModeling – Naïve vs. Improved\n    a.\tFit a baseline OLS model without confounders.\n    b.\tFit an improved OLS model with transformations, confounders, and interactions.\n    c.\tCompare coefficients, R2, AIC/BIC.\n5.\tAssumptions Diagnostics\n    a.\tCheck linearity, normality, and homoscedasticity (residual plots, QQ plot, Breusch–Pagan).\n    b.\tEvaluate multicollinearity with VIF.\n6.\tOutliers & Influence\n    a.\tIdentify leverage points and high Cook’s D values.\n    b.\tDiscuss whether to drop or retain influential cases.\n    c.\tRefit model and compare metrics.\n7.\tGeneralization Check\n    a.\tTrain/test split; calculate RMSE on test data.\n    b.\tReflect on in-sample vs. out-of-sample fit.\n8.\tThink About:\n    a.\tWhich variables appear most predictive of incidents?\n    b.\tHow do confounders shift interpretations?\n    c.\tWhat model would you present to leadership—and what caveats remain?\n"
  },
  {
   "cell_type": "markdown",
   "id": "89cde704-a723-4a4e-ac99-e573a1de2523",
   "metadata": {
    "name": "DataFamiliarization",
    "collapsed": false
   },
   "source": "1.\tData Familiarization & Hygiene\n    a.\tInspect schema, ranges, and datatypes.\n    b.\tDetect and fix issues: missing values, impossible percentages, and text in numeric fields.\n    c.\tApply simple imputation and clipping strategies.\n\n"
  },
  {
   "cell_type": "code",
   "id": "d4eb829d-69b2-4e62-9400-360f38b667f9",
   "metadata": {
    "language": "python",
    "name": "DataFamiliarizationCode"
   },
   "outputs": [],
   "source": "#load the csv into a data frame\n#perform some familiarization steps with the data set\n#inspect and display some meta/summary data about the data set\n\nimport pandas as pd\n\n#load CSV to pandas data frame\ncyber_df = pd.read_csv(\"Lab 05/cybersec.csv\")\n\n# 1a. Inspect schema and types\nprint(\"Schema and Data Types:\\n\", cyber_df.dtypes, \"\\n\")\n\n#show summary statistics\nprint(\"Summary Statistics:\\n\", cyber_df.describe(include='all'), \"\\n\")\n\n#show missing values\nprint(\"Missing Values:\\n\", cyber_df.isnull().sum(), \"\\n\")\n\n#show some unique values for object columns\nobject_columns = cyber_df.select_dtypes(include=['object']).columns\nfor col in object_columns:\n    print(f\"{col} unique values:\", cyber_df[col].unique()[:10])\nprint()\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48570337-ffb8-4e9e-ae61-1238fc69b666",
   "metadata": {
    "language": "python",
    "name": "DataCleanUp"
   },
   "outputs": [],
   "source": "# 1b. Detect & fix issues\n#clip impossible % values, just in case\ncyber_df['phishing_sim_click_rate'] = cyber_df['phishing_sim_click_rate'].clip(upper=1.0)\n\n#fix text 'null' if present (not needed here, but included as good hygiene)\ncyber_df.replace(\"null\", pd.NA, inplace=True)\n\n# 1c. setup the missing values (mean)\ncyber_df['vuln_count'] = cyber_df['vuln_count'].fillna(cyber_df['vuln_count'].mean())\ncyber_df['training_completion_rate'] = cyber_df['training_completion_rate'].fillna(cyber_df['training_completion_rate'].mean())\n\n#check if all missing values handled\nprint(\"Post-cleaning Missing Values:\\n\", cyber_df.isnull().sum())\n\n#quick preview\ncyber_df.head()\n\n#actual column names\nprint(cyber_df.columns.tolist())\n\n#check the min and max values of the endpoint_coverage column\nmin_coverage = cyber_df['endpoint_coverage'].min()\nmax_coverage = cyber_df['endpoint_coverage'].max()\n\nprint(f\"Endpoint Coverage ranges from {min_coverage}% to {max_coverage}%.\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2fbb9f3b-4b7e-42ef-8d54-a69247b61682",
   "metadata": {
    "name": "EDA",
    "collapsed": false
   },
   "source": "2.\tExploratory Analysis (EDA)\n    a.\tPlot distributions (incidents histogram).\n    b.\tExplore relationships (incidents vs. vulnerabilities, patch time).\n    c.\tSpot skewness and potential transformations.\n"
  },
  {
   "cell_type": "code",
   "id": "31c8df66-7ef6-48e1-b3c6-bf722ba30f94",
   "metadata": {
    "language": "python",
    "name": "EDACode2a"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import skew\n\n#2a: Histogram of Security Incidents\n\nplt.figure(figsize=(8, 5))\nsns.histplot(cyber_df['security_incidents'], bins=30, kde=True)\nplt.title(\"Histogram of Security Incidents (Original)\")\nplt.xlabel(\"Number of Security Incidents\")\nplt.ylabel(\"Frequency\")\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4891591-357c-4b26-bfcf-0fb479569c60",
   "metadata": {
    "language": "python",
    "name": "EDACode2b"
   },
   "outputs": [],
   "source": "#2b: Explore relationships via scatter plots\n\n#incidents vs. vuln_count\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x='vuln_count', y='security_incidents', data=cyber_df)\nplt.title(\"Security Incidents vs. Vulnerability Count\")\nplt.xlabel(\"Vulnerability Count\")\nplt.ylabel(\"Security Incidents\")\nplt.show()\n\n#incidents vs. mean_time_to_patch\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x='mean_time_to_patch', y='security_incidents', data=cyber_df)\nplt.title(\"Security Incidents vs. Mean Time to Patch\")\nplt.xlabel(\"Mean Time to Patch (days)\")\nplt.ylabel(\"Security Incidents\")\nplt.show()\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d073434f-5ef6-4c44-b138-75710ea23927",
   "metadata": {
    "language": "python",
    "name": "EDACode2c"
   },
   "outputs": [],
   "source": "#2c: Skewness and Log Transformation\n\n#original skewness\norig_skew = skew(cyber_df['security_incidents'].dropna())\nprint(f\"Skewness of security_incidents (original): {orig_skew:.2f}\")\n\n#log transform\ncyber_df['log_security_incidents'] = np.log1p(cyber_df['security_incidents'])\n\n#skewness after transformation\nlog_skew = skew(cyber_df['log_security_incidents'].dropna())\nprint(f\"Skewness after log transformation: {log_skew:.2f}\")\n\n#plot log-transformed histogram\nplt.figure(figsize=(8, 5))\nsns.histplot(cyber_df['log_security_incidents'], bins=30, kde=True)\nplt.title(\"Histogram of Log-Transformed Security Incidents\")\nplt.xlabel(\"log(1 + Security Incidents)\")\nplt.ylabel(\"Frequency\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5402025d-d75a-4508-886e-5eaf2dfaa249",
   "metadata": {
    "name": "Confounders",
    "collapsed": false
   },
   "source": "3.\tFeature Engineering & Confounders\n    a.\tCreate transformations (log_org_size, log_vulns).\n    b.\tEngineer interaction terms (e.g., vulnerabilities × endpoint coverage gaps).\n    c.\tDiscuss how org size and budget confound relationships.\n"
  },
  {
   "cell_type": "code",
   "id": "1ae368a9-a17d-4041-aa31-acecf560c0dc",
   "metadata": {
    "language": "python",
    "name": "ConfoundersCode3a"
   },
   "outputs": [],
   "source": "#create log-transformed columns\n\ncyber_df['log_org_size'] = np.log1p(cyber_df['org_size'])\ncyber_df['log_vulns'] = np.log1p(cyber_df['vuln_count'])\n\n#visualize\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(cyber_df['log_org_size'], bins=30, kde=True, ax=axs[0])\naxs[0].set_title(\"Log-transformed Org Size\")\n\nsns.histplot(cyber_df['log_vulns'], bins=30, kde=True, ax=axs[1])\naxs[1].set_title(\"Log-transformed Vulnerability Count\")\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8592868-1c75-40a8-8843-675693fcf9e2",
   "metadata": {
    "language": "python",
    "name": "ConfoundersCode3b"
   },
   "outputs": [],
   "source": "#create interaction term\n#cyber_df['vuln_x_endpoint'] = cyber_df['vuln_count'] * (1 - cyber_df['endpoint_coverage'])\n#print(cyber_df['endpoint_coverage'])\n\n#visualize\n#sns.scatterplot(x='vuln_x_endpoint', y='security_incidents', data=cyber_df)\n#plt.title(\"Security Incidents vs. Vulnerability × Endpoint Gap Interaction\")\n#plt.xlabel(\"Vulnerability × Endpoint Gap\")\n#plt.ylabel(\"Security Incidents\")\n#plt.show()\n\n#endpoint_coverage_gap if not already done\ncyber_df['endpoint_coverage_gap'] = 100 - cyber_df['endpoint_coverage']\n\n#scatterplot to show the relationship\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=cyber_df, x='endpoint_coverage', y='endpoint_coverage_gap')\n\n#add reference line (y = 100 - x)\nplt.plot([0, 100], [100, 0], color='red', linestyle='--', label='Gap = 100 - Coverage')\n\n#labels and legend\nplt.title('Endpoint Coverage vs. Coverage Gap')\nplt.xlabel('Endpoint Coverage (%)')\nplt.ylabel('Coverage Gap (%)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "46dc182d-b5e0-4f55-a46c-d8652fd40469",
   "metadata": {
    "name": "ConfoundingDiscussion3c",
    "collapsed": false
   },
   "source": "3c.\tDiscuss how org size and budget confound relationships.\n\nOrganizational size (org_size) and budget (it_budget_per_emp) are likely confounding variables in this dataset. Larger organizations typically:\n\t•\tHave higher vulnerability counts due to more assets\n\t•\tSpend more on cybersecurity (not necessarily per employee, but in total)\n\t•\tMay still experience more incidents simply due to scale -\n        More humans = more assets = more vulnerabilities = more RISK!\n\nWe should account for these confounders to avoid biased interpretations, such as overestimating the impact of vulnerabilities or underestimating the effectiveness of a security budget. Log-transforming org_size and including it as a predictor helps address nonlinear scaling and reduces skewness."
  },
  {
   "cell_type": "markdown",
   "id": "2aa5f112-7c4f-4acf-b8bb-5aa9ea323c35",
   "metadata": {
    "name": "Modeling",
    "collapsed": false
   },
   "source": "4.\tModeling – Naïve vs. Improved\n    a.\tFit a baseline OLS model without confounders.\n    b.\tFit an improved OLS model with transformations, confounders, and interactions.\n    c.\tCompare coefficients, R2, AIC/BIC.\n"
  },
  {
   "cell_type": "code",
   "id": "09976fa7-7aad-4d79-a8ee-34f579d65b11",
   "metadata": {
    "language": "python",
    "name": "ModelingCode4a"
   },
   "outputs": [],
   "source": "#4.\tModeling – Naïve vs. Improved\n#    a.\tFit a baseline OLS model without confounders.\n        \nimport statsmodels.api as sm\n\n#define X and y for the baseline model (no confounders)\nX_base = cyber_df[['vuln_count', 'mean_time_to_patch', 'endpoint_coverage_gap']]\ny = cyber_df['security_incidents']\n\n#add a constant to the predictors (for intercept)\nX_base_const = sm.add_constant(X_base)\n\n#fit the baseline model\nbaseline_model = sm.OLS(y, X_base_const).fit()\n\n#print model summary\nprint(baseline_model.summary())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c27ffb6f-cfa9-43a1-922c-e063f6925e83",
   "metadata": {
    "language": "python",
    "name": "ModelingCode4b"
   },
   "outputs": [],
   "source": "#  b.Fit an improved OLS model with transformations, confounders, \n#    and interactions.\n\n#transformations\ncyber_df['log_org_size'] = np.log(cyber_df['org_size'])\ncyber_df['log_vuln_count'] = np.log(cyber_df['vuln_count'] + 1)  # +1 to avoid log(0)\n\n# Interaction term\ncyber_df['vuln_x_gap'] = cyber_df['vuln_count'] * cyber_df['endpoint_coverage_gap']\n\ncyber_df_original = cyber_df.copy() #in case we need the original later\n\n#define X and y for improved model ---\npredictors = [\n    'log_vuln_count',\n    'mean_time_to_patch',\n    'endpoint_coverage_gap',\n    'vuln_x_gap',\n    'log_org_size',\n    'it_budget_per_emp'\n]\nX_improved = cyber_df[predictors]\ny = cyber_df['security_incidents']\n\n#add constant (intercept)\nX_improved_const = sm.add_constant(X_improved)\n\n#fit model\nimproved_model = sm.OLS(y, X_improved_const).fit()\n\n#show summary\nprint(improved_model.summary())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9cbe8b32-4af9-413e-a817-41eedff2a638",
   "metadata": {
    "name": "ModelingCommentary4c",
    "collapsed": false
   },
   "source": "Compare coefficients, R2, AIC/BIC.\n\nThe improved model performs better across R², AIC, and BIC.  It also includes meaningful new predictors (log_vuln_count, log_org_size, interaction terms)"
  },
  {
   "cell_type": "code",
   "id": "e7d8c793-bf24-45c8-a064-558660c7c4c7",
   "metadata": {
    "language": "python",
    "name": "ModelingCommentary4cVisualization"
   },
   "outputs": [],
   "source": "#the dessired model comparison metrics\nmetrics = ['R-squared', 'Adjusted R²', 'AIC', 'BIC']\n\n#setup baseline and improved model values to plot\nbaseline_values = [baseline_model.rsquared, \n                   baseline_model.rsquared_adj, \n                   baseline_model.aic, \n                   baseline_model.bic]\nimproved_values = [improved_model.rsquared, \n                   improved_model.rsquared_adj, \n                   improved_model.aic, \n                   improved_model.bic]\n\n#create a compoarison DataFrame\ncomparison_df = pd.DataFrame({\n    'Metric': metrics,\n    'Baseline Model': baseline_values,\n    'Improved Model': improved_values\n})\n\n#try a plot\nfig, axs = plt.subplots(1, 2, figsize=(14, 5))\n\n#plot R² comparison\ncomparison_df.iloc[:2].plot(x='Metric', kind='bar', ax=axs[0], color=['#1f77b4', '#2ca02c'])\naxs[0].set_title('R² Comparison')\naxs[0].set_ylabel('Score')\naxs[0].legend(loc='upper left')\n\n#plot AIC/BIC comparison\ncomparison_df.iloc[2:].plot(x='Metric', kind='bar', ax=axs[1], color=['#1f77b4', '#2ca02c'])\naxs[1].set_title('AIC/BIC Comparison')\naxs[1].set_ylabel('Score (Lower is Better)')\naxs[1].legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32ceb687-fb45-49b5-b0c0-97aabee82f21",
   "metadata": {
    "name": "ModelingCommentary4cContinued",
    "collapsed": false
   },
   "source": "I thought I'd try the visual comparison, but it didn't work well due to the scale.  I left it in, just so you can see the steps I took.  Here are some written thoughts on the model differences.\n\nNext, I'll try to print these in a table for better observation."
  },
  {
   "cell_type": "code",
   "id": "b6f112a0-6746-4897-8a15-52a12274b309",
   "metadata": {
    "language": "python",
    "name": "ModelingCommentary4cTable"
   },
   "outputs": [],
   "source": "#metric names\nmetrics = ['R²', 'Adjusted R²', 'AIC', 'BIC']\n\n#create a comparison DataFrame\ncoefficient_df = pd.DataFrame({\n    'Metric': metrics,\n    'Baseline Model': baseline_values,\n    'Improved Model': improved_values\n})\n\n#format numbers for readability\ncoefficient_df['Baseline Model'] = coefficient_df['Baseline Model'].apply(lambda x: round(x, 3) if isinstance(x, float) else x)\ncoefficient_df['Improved Model'] = coefficient_df['Improved Model'].apply(lambda x: round(x, 3) if isinstance(x, float) else x)\n\n#display the coefficient comparison table\nprint(coefficient_df)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92b7793a-d441-448e-8337-626aaece639d",
   "metadata": {
    "name": "ModelingCommentary4cFinal",
    "collapsed": false
   },
   "source": "Thoughts on comparing the two models:\n\nR2 base is 0.255, improved is 0.272 which is a slight improvement.  The improved model explains 2% more variance in the outcome (security_incidents) than the baseline model.\n\nAdjusted R2 base is 0.246, improved is 0.253 which is a consistent improvement: Adjusted R² also increases, suggesting that the additional predictors in the improved model are beneficial after accounting for model complexity.\n\nAIC base is 919.032, improved is 919.461, which is only a slight increase. This usually suggests a worse model, but the difference is so small (~0.4) that it is likely statistically insignificant.\n\nBIC base is 932.955, improved is 943.825, which is higher in the (presumably) improved model: BIC penalizes complexity more heavily than AIC. The increase (~11 points) indicates that the improved model may be too complex for the added benefit.\n"
  },
  {
   "cell_type": "markdown",
   "id": "cad5210d-e40c-447c-bcd7-5c0794aef91c",
   "metadata": {
    "name": "AssumptionsQ5",
    "collapsed": false
   },
   "source": "5.\tAssumptions Diagnostics\n    a.\tCheck linearity, normality, and homoscedasticity (residual plots, QQ plot, Breusch–Pagan).\n    b.\tEvaluate multicollinearity with VIF.\n"
  },
  {
   "cell_type": "code",
   "id": "e8672ae4-a9be-4433-b8b6-3538b0689418",
   "metadata": {
    "language": "python",
    "name": "AssumptionsCodeQ5a"
   },
   "outputs": [],
   "source": "\n#setup components\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nfrom statsmodels.stats.outliers_influence import OLSInfluence\nfrom scipy import stats\n\n#get residuals and fitted values from improved model\nresiduals = improved_model.resid\nfitted = improved_model.fittedvalues\n\n#linearity & Homoscedasticity\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x=fitted, y=residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.title(\"Residuals vs Fitted (Linearity & Homoscedasticity)\")\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\n#Normality: Q-Q Plot\nsm.qqplot(residuals, line='s')\nplt.title(\"Q-Q Plot (Normality of Residuals)\")\nplt.show()\n\n#Breusch–Pagan Test for Homoscedasticity\n#requires residuals and independent variables\nX_improved_const = improved_model.model.exog\nbp_test = het_breuschpagan(residuals, X_improved_const)\n\nlabels = ['Lagrange multiplier statistic', 'p-value', \n          'f-value', 'f p-value']\nprint(\"\\nBreusch–Pagan Test Results:\")\nfor name, value in zip(labels, bp_test):\n    print(f\"{name}: {value:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d01a58c8-5167-4e37-ae74-1865f4040712",
   "metadata": {
    "language": "python",
    "name": "AssumptionsVIF",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#assume X_improved_const is your design matrix with intercept term already added\n#if not already done:\nX_improved_const = sm.add_constant(X_improved)\n\n#create a DataFrame for VIF values\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X_improved_const.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_improved_const, i) \n                   for i in range(X_improved_const.shape[1])]\n\n#format all float values to 3 decimal places\npd.set_option(\"display.float_format\", \"{:.3f}\".format)\n\n#display VIF table\nprint(vif_data)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4b850ce-9833-4501-a36c-311da5539a31",
   "metadata": {
    "name": "AssumptionsVIFCommentary",
    "collapsed": false
   },
   "source": "log_vuln_count has a high multicollinearity which is likely a problem \n\nlog_org_size is just barely showing high multicollinearity, which should be investigated further\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "e47bb093-3a35-4c62-9544-83d9853e0159",
   "metadata": {
    "name": "Outliers",
    "collapsed": false
   },
   "source": "6.\tOutliers & Influence\n    a.\tIdentify leverage points and high Cook’s D values.\n    b.\tDiscuss whether to drop or retain influential cases.\n    c.\tRefit model and compare metrics.\n"
  },
  {
   "cell_type": "code",
   "id": "ea1bf099-da69-41c3-bf21-5633599a7839",
   "metadata": {
    "language": "python",
    "name": "OutliersCodeCooksD"
   },
   "outputs": [],
   "source": "#using our improved model\ninfluence = improved_model.get_influence()\n\n#leverage values\nleverage = influence.hat_matrix_diag\n\n#Cook's Distance\ncooks_d, _ = influence.cooks_distance\n\n# Create a summary DataFrame\ninfluence_df = pd.DataFrame({\n    \"Leverage\": leverage,\n    \"Cooks_D\": cooks_d\n})\n\n#flag high leverage: rule of thumb: leverage > 2k/n\nn = X_improved_const.shape[0]\nk = X_improved_const.shape[1]\nleverage_threshold = 2 * k / n\ninfluence_df[\"High_Leverage\"] = influence_df[\"Leverage\"] > leverage_threshold\n\n#flag high Cook's D: rule of thumb: Cook's D > 4/n\ncooks_d_threshold = 4 / n\ninfluence_df[\"High_CooksD\"] = influence_df[\"Cooks_D\"] > cooks_d_threshold\n\n#display top suspicious points\nprint(influence_df.sort_values(\"Cooks_D\", ascending=False).head(10))\n\n#visualize Cook’s D and leverage\nplt.figure(figsize=(10, 5))\n#plt.stem(np.arange(n), cooks_d, markerfmt=\",\", basefmt=\" \", use_line_collection=True)\nplt.stem(np.arange(n), cooks_d, markerfmt=\",\", basefmt=\" \")\nplt.axhline(y=cooks_d_threshold, color='r', linestyle='--', label=\"Cook's D Threshold\")\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Cook's Distance by Observation\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 5))\nplt.scatter(leverage, cooks_d, alpha=0.6)\nplt.axhline(y=cooks_d_threshold, color='red', linestyle='--', label=\"Cook's D Threshold\")\nplt.axvline(x=leverage_threshold, color='orange', linestyle='--', label=\"Leverage Threshold\")\nplt.xlabel(\"Leverage\")\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Influence Plot (Leverage vs Cook's D)\")\nplt.legend()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7613741-ce66-4dbf-b42b-d9483af84a7d",
   "metadata": {
    "name": "DropOrRetain",
    "collapsed": false
   },
   "source": "6b.\tDiscuss whether to drop or retain influential cases.\n  \nAll 10 points have Cook’s D values that exceed the 4/n threshold, so they are considered influential.\n\t•\tOnly 4 of the 10 also have high leverage:\n\t•\tIndexes: 208, 39, 186, 129\n\t•\tSome points like 106 or 200 have low leverage but still show moderate influence (via Cook’s D).\n\n"
  },
  {
   "cell_type": "code",
   "id": "627a6a6d-5ed5-44d7-838b-9d03a6c95ee9",
   "metadata": {
    "language": "python",
    "name": "DropOrRetainCode"
   },
   "outputs": [],
   "source": "#reload our dataset and prepare X and y\nnew_cyber_df = cyber_df_original.copy()\n\ny = new_cyber_df[\"security_incidents\"]\nX = new_cyber_df[[\"log_vuln_count\", \"mean_time_to_patch\", \"endpoint_coverage\", \n        \"vuln_x_gap\", \"log_org_size\", \"it_budget_per_emp\"]]\nX_const = sm.add_constant(X)\n\nmodel_raw = sm.OLS(y_refit, X_refit_const).fit()\nprint(model_raw.summary())\n\n#fit the original model\nmodel_full = sm.OLS(y, X_const).fit()\n\n#get influence measures (Cook’s D and leverage)\ninfluence = model_full.get_influence()\ncooks_d = influence.cooks_distance[0]\nleverage = influence.hat_matrix_diag\n\n#create a DataFrame of influence measures\ninfluence_df = pd.DataFrame({\n    'leverage': leverage,\n    'cooks_d': cooks_d\n})\n\n#define thresholds\nn = X_const.shape[0]\np = X_const.shape[1]\nleverage_thresh = 2 * p / n\ncooksd_thresh = 4 / n\n\n#identify points to drop\nto_drop = influence_df[(influence_df['leverage'] > leverage_thresh) &\n                       (influence_df['cooks_d'] > cooksd_thresh)].index\n\n#drop those rows\ndf_refit = new_cyber_df.drop(index=to_drop)\n\n#refit the model\ny_refit = df_refit[\"security_incidents\"]\nX_refit = df_refit[[\"log_vuln_count\", \"mean_time_to_patch\", \"endpoint_coverage_gap\", \n                    \"vuln_x_gap\", \"log_org_size\", \"it_budget_per_emp\"]]\nX_refit_const = sm.add_constant(X_refit)\n\nmodel_refit = sm.OLS(y_refit, X_refit_const).fit()\n\n#print refitted model summary\nprint(model_refit.summary())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3602c0fc-2848-4ae1-8062-db53acef5afb",
   "metadata": {
    "name": "Generalization",
    "collapsed": false
   },
   "source": "7.\tGeneralization Check\n    a.\tTrain/test split; calculate RMSE on test data.\n    b.\tReflect on in-sample vs. out-of-sample fit.\n"
  },
  {
   "cell_type": "code",
   "id": "cc18fd08-522e-4fe3-82c0-1b74c178d261",
   "metadata": {
    "language": "python",
    "name": "TrainTestSplit"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n#define features and target again\nfeatures = [\"log_vuln_count\", \"mean_time_to_patch\", \"endpoint_coverage_gap\", \n            \"vuln_x_gap\", \"log_org_size\", \"it_budget_per_emp\"]\ntarget = \"security_incidents\"\n\nX = cyber_df[features]\ny = cyber_df[target]\n\n#add constant for statsmodels OLS\nX_const = sm.add_constant(X)\n\n#split the data: 80% train, 20% test\nX_train, X_test, y_train, y_test = train_test_split(X_const, y, test_size=0.2, random_state=42)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56010ae8-2527-45e6-b4cf-5518ff089e2c",
   "metadata": {
    "language": "python",
    "name": "FitModel"
   },
   "outputs": [],
   "source": "#fit model on training data\nmodel_train = sm.OLS(y_train, X_train).fit()\n\n#predict on test data\ny_pred_test = model_train.predict(X_test)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da7939fd-8c03-4800-8e83-bb12dd7ec6aa",
   "metadata": {
    "language": "python",
    "name": "RMSE"
   },
   "outputs": [],
   "source": "#calculate RMSE for test data\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n#also calculate RMSE on training data for comparison\ny_pred_train = model_train.predict(X_train)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n\nprint(f\"RMSE (Train): {rmse_train:.2f}\")\nprint(f\"RMSE (Test):  {rmse_test:.2f}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48fc9b44-82af-4e5e-a2bf-e8003534897c",
   "metadata": {
    "name": "RMSEReflection",
    "collapsed": false
   },
   "source": "RMSE results indicate:\n\t•\tRMSE (Train): 1.64\n\t•\tRMSE (Test): 1.45\n\nMy Training RMSE is 1.64 and my Test RMSE is 1.45. It is somewhat for the Test RMSE to be lower than the Training RMSE, but not necessarily a problem. This suggests my model is not overfitting and seems to generalize reasonably well to unseen data.  We may have too small of a sample size to conduct the RMSE analysis with any confidene.\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "67b8e600-d8cb-4850-8160-51203f3ab874",
   "metadata": {
    "name": "ReflectOnFit",
    "collapsed": false
   },
   "source": "In-Sample Fit (Training RMSE = 1.64):\n\t•\tThis represents how well the model performs on data it was trained on.\n\t•\tOur training RMSE of 1.64 indicates that the model can predict reasonably well within the sample it learned from.\n\t•\tHowever, relying solely on this metric can be misleading if the model is overfitting (i.e., too closely matching the training data).\n\nOut-of-Sample Fit (Test RMSE = 1.45):\n\t•\tThis measures performance on unseen data, which better reflects how the model might perform in the real world.\n\t•\tOur test RMSE of 1.45 is slightly better than the training RMSE, suggesting the model generalizes well.\n\t•\tThis is a positive sign — the model is not just memorizing the training data but capturing real patterns that apply to new data.\n\n\nThis tells me:\n\t•\tNo overfitting is apparent. If the test RMSE were much higher than the training RMSE, that would indicate poor generalization.\n\t•\tThe model may even be slightly underfitting, or the test set might be less noisy than the training set (easier to predict).\n\t•\tModel performance is stable across datasets — a very good sign.\n    \n\n\nOur model’s performance is consistent and generalizes well from training to test data. You could now:\n\t•\tReport this as a strong result.\n\t•\tConsider further improvements (e.g., feature engineering or regularization).\n\t•\tOr stop here if the goal was to build a reasonably accurate and interpretable model.\n"
  },
  {
   "cell_type": "markdown",
   "id": "c67fa342-22a4-4d20-9a35-296762a42caf",
   "metadata": {
    "name": "ThinkAbout",
    "collapsed": false
   },
   "source": "8.\tThink About:\n    a.\tWhich variables appear most predictive of incidents?\n    b.\tHow do confounders shift interpretations?\n    c.\tWhat model would you present to leadership—and what caveats remain?"
  },
  {
   "cell_type": "markdown",
   "id": "5b910b4f-6f61-4ca4-8b55-cea7b748cf9d",
   "metadata": {
    "name": "ThinkAbut8a",
    "collapsed": false
   },
   "source": "8.\tThink About:\n    a.\tWhich variables appear most predictive of incidents?\n\nendpoint_coverage (well, the gap) is one of the strongest predictors. A positive and significant coefficient indicates that departments with larger endpoint security gaps tend to have more incidents. Makes sense — more unprotected endpoints = more risk.\n\nlog_vuln_count is statistically significant with a negative coefficient. This might seem counterintuitive, but in log-linear models, this can reflect diminishing returns — or that logged values are smoothing large differences. Still, it shows vulnerability volume relates to incidents.\n\nvuln_x_gap (interaction term)\nStrong, significant, and positive. Indicates that vulnerabilities are especially risky when endpoint coverage is also low. This interaction captures a compounding effect and is crucial for understanding risk.\n\nlog_org_size is marginally significant. Also mekes sense that larger organizations (even log-transformed) appear more likely to report more incidents. Could be due to scale: more devices, more staff, more chances for attack.\n\nmean_time_to_patch and it_budget_per_emp\nThese were not statistically significant in our model — suggesting they may not be reliable standalone predictors in this dataset.\n"
  },
  {
   "cell_type": "markdown",
   "id": "99f79ccb-12a6-4d9f-a592-a995b43f0b19",
   "metadata": {
    "name": "ThinkAbout8b",
    "collapsed": false
   },
   "source": "8.\tThink About:\n    b.\tHow do confounders shift interpretations?\n\nWith the addition of confounder control the relationships became more accurate and controllable.  This gave us better insights, which leads to more effective and efficient prioritization and utliization of our valuable resources.\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "c70a295e-c4b3-4a60-8eda-c10c0aeaf478",
   "metadata": {
    "name": "ThinkAbout8c",
    "collapsed": false
   },
   "source": "Think About:\n    c.\tWhat model would you present to leadership—and what caveats remain?\n\nWhat Model Would I Present to Leadership?\n\nI would present the Improved OLS Regression Model that includes:\n\t•\tLog-transformed predictors (e.g., log_vuln_count, log_org_size)\n\t•\tInteraction term (vuln_x_gap) to capture compounding risk\n\t•\tKey confounders like org_size and it_budget_per_emp\n\t•\tThe endpoint_coverage_gap variable, which had one of the strongest coefficients (which seems to be obvious)\n\nThis model:\n\t•\tExplains more variance than the naïve model (higher R2 and adjusted R2)\n\t•\tHas lower AIC/BIC values, indicating a better model fit\n\t•\tUses interpretable variables that align with operational and strategic priorities\n\t•\tProvides actionable insight into the areas most predictive of incident risk (like gaps in endpoint coverage and high vulnerability load)\n\n\nWhat Caveats Remain?\n\nDespite the model improvements, several limitations should be transparently communicated:\n\n1. Causality Is Not Guaranteed\n\t•\tThe model identifies associations, not causal relationships.\n\t•\tLeadership should not infer direct cause-effect (e.g., “fixing endpoint gaps will guarantee fewer incidents”).  This point should be clearly driven home. This is a \"risk tolerance\" conversation, not an absolute security conversation.\n\n2. Potential Data Quality Issues\n\t•\tSome predictors required imputation or transformation (e.g., missing values, skewed distributions).\n\t•\tThe data may still contain biases, inconsistencies, or outliers.  Although, most of the data is derived from raw logs. This reduces the likelihood of error in the original data captured.\n\n3. Outliers & Influential Cases\n\t•\tSome influential data points (high leverage or Cook’s D) exist.\n\t•\tWhile these were identified and could be excluded for robustness checks, real-world decisions must consider whether those departments are edge cases or high-risk units. Let's not lose sight of the fact we are balancing investment against risk. There may not be a clear \"formula\" to apply to a final decision. Our job is to get the best information we can \"in the language of risk\" so decision makers can make INFORMED decisions and not just \"hope\" for bad things to not happen.\n\n4. Multicollinearity Risk\n\t•\tSome variables show moderate multicollinearity (e.g., log_org_size may correlate with budget or vuln_count).\n\t•\tWhile VIF scores were below critical thresholds, they should be monitored in future models.\n\n5. Need for Continuous Validation\n\t•\tThe threat landscape evolves rapidly.\n    •\tThis is NOT a one-time evaluation.  It should be performed with a recurring frequency - leading to different decisions based on the updated outcomes.\n\t•\tRecommend setting up a model retraining schedule (e.g., quarterly) as new incident and risk data is collected.\n\nMy Final Recommendation to Leadership:\n\n“This model helps us quantify and prioritize cybersecurity risk across departments. The biggest drivers of incidents appear to be gaps in endpoint coverage and volume of vulnerabilities, especially when both are present. However, we should treat this model as a decision-support tool, not an absolute truth. Regular updates, ongoing data hygiene, and domain expertise should continue to inform our cyber risk strategy.”\n"
  }
 ]
}